<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Title | Jae Kim - Data Science Blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Title" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Explore and learn data science concepts with me." />
<meta property="og:description" content="Explore and learn data science concepts with me." />
<link rel="canonical" href="https://disney-snoopy.github.io/fastbook/2021/02/06/Linear_regression_probability.html" />
<meta property="og:url" content="https://disney-snoopy.github.io/fastbook/2021/02/06/Linear_regression_probability.html" />
<meta property="og:site_name" content="Jae Kim - Data Science Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-02-06T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"Explore and learn data science concepts with me.","url":"https://disney-snoopy.github.io/fastbook/2021/02/06/Linear_regression_probability.html","@type":"BlogPosting","headline":"Title","dateModified":"2021-02-06T00:00:00-06:00","datePublished":"2021-02-06T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://disney-snoopy.github.io/fastbook/2021/02/06/Linear_regression_probability.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/fastbook/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://disney-snoopy.github.io/fastbook/feed.xml" title="Jae Kim - Data Science Blog" /><link rel="shortcut icon" type="image/x-icon" href="/fastbook/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/fastbook/">Jae Kim - Data Science Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/fastbook/about/">About Me</a><a class="page-link" href="/fastbook/search/">Search</a><a class="page-link" href="/fastbook/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Title</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-02-06T00:00:00-06:00" itemprop="datePublished">
        Feb 6, 2021
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      9 min read
    
</span></p>

    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/disney-snoopy/fastbook/tree/master/_notebooks/2021-02-06-Linear_regression_probability.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/fastbook/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/disney-snoopy/fastbook/master?filepath=_notebooks%2F2021-02-06-Linear_regression_probability.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/fastbook/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/disney-snoopy/fastbook/blob/master/_notebooks/2021-02-06-Linear_regression_probability.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/fastbook/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-02-06-Linear_regression_probability.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Motivation">Motivation<a class="anchor-link" href="#Motivation"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Linear regression has been around for centuries. Gauss used linear regression for the prediction of planetary movement in early 1800s. Since then, numerous models have been invented and many of them perform significantly better than linear regression. However, linear regression remains to be relevant due to its simplicity and interpretability.</p>
<p>Linear regression enables clear statistical inference on the model and its coefficients. When story telling is more important than better performance metrics, linear regression can be your tool of choice. For example, when a CEO wants to make a business decision based on data, he would still want to understand the reasoning behind the decision. Although complex black box models may have better performance, the CEO would not be comfortable with making decisions based on the models he doesn't understand. On the other hand, linear regression can clearly outline how each feature in the model is estimated to affect the model prediction. The CEO would be much more comfortable with following the model that he actually understands.</p>
<p>Furthermore, linear regression has a closed form solution (can be solved without iterative steps) which ensures the consistent outcome. It is also a fundamental building block in more complex models, so there is no harm in spending some time understanding it!</p>
<p>By the end of the post, you will understand why mean squared error (MSE) is used among other metrics to find the best fit line. Also, you will understand the update rule we used to find regression coefficients when coding our own gradient descent algorithm!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Derivation">Derivation<a class="anchor-link" href="#Derivation"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Linear regression can be derived in many forms. I find the derivation based on the maximum likelihood estimation (MLE) of normally distributed residuals to be most intuitive. So that's the one I will introduce.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Derivation-of-Likelihood-Function">Derivation of Likelihood Function<a class="anchor-link" href="#Derivation-of-Likelihood-Function"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's assume the followings:</p>
<ol>
<li>linearity and additivity of the relationship between dependent and independent variables</li>
<li>normality of the error distribution.</li>
</ol>
<p>From the linearity and additivity assumptions, we can model our hypothesis function as a linear function of dependent vairables:</p>
$$
\begin{align}
h(x)  
&amp; = \theta_0 + \theta_1x_1 + \cdots + \theta_nx_n\\
&amp; = \sum_{j=0}^{d} \theta_jx_j\\
&amp; = \theta^Tx
\end{align}
$$<p>where the $\theta$s are the regression coefficients and $x_i$ is the $i^{th}$ feature. Note that $\theta_0$ is the intercept term. d is the number of features.</p>
<p>As in the housing price prediction challenge, $h(x)$ is the predicted price of the house, $\theta_1$ is the general living area, $\theta_2$ is the number of bedrooms and so on.</p>
<p>Using the hypothesis function, we can now model how the independent vairable and the dependent variables are related:
$$
\begin{equation}
y^i = \theta^Tx^{i} + \epsilon^{i}
\end{equation}
$$
where $\epsilon^{i}$ is an error that captures random noise and unmodelled effects. Let's further assume that $\epsilon^{i}$ are distributed according to a Gaussian distribution (Normal distribution) and are IID (independently and identically distributed) with mean zero and variance $\sigma^2$. IID implies that residuals are not related to each other and they are sampled from the same distribution. The justification for assuming residual normality is based on central limit theorem. For more in-depth intuition on why so many things in the world are distributed according to Normal distribution, hence the name Normal, refer to this <a href="https://academic.oup.com/bjps/article-abstract/65/3/621/1497281?redirectedFrom=fulltext">paper</a>.</p>
<p>Above assumptions can be written in mathematical notations:</p>
$$
\begin{equation}
\epsilon^{i} \sim  \mathcal{N}(0, \sigma^2)
\end{equation}
$$<p>The probability density function (pdf) of Gaussian distribution is given by
$$
\begin{equation}
p(\epsilon^{i}) = \frac{1}{\sigma \sqrt{2\pi}} exp \bigl(-\frac{(\epsilon^{i})^2}{2\sigma^2}\bigr)
\end{equation}
$$
from equation 4 and 6,
$$
\begin{equation}
p(y^{(i)}|x^{(i)};\theta) = \frac{1}{\sigma \sqrt{2\pi}} exp \bigl(-\frac{(y^{i}-\theta^Tx^{i})^2}{2\sigma^2}\bigr)
\end{equation}
$$</p>
<p>Let's examine what equation 7 implies. $p(y^{(i)}|x^{(i)};\theta)$ indicates that it's the distribution of y given x and it is parameterised by $\theta$. From a set of assumptions, we have described the probability of y in terms of x and $\theta$. This can be also expressed as a distribution of y given x:
$$
\begin{equation}
y^{(i)}|x^{(i)};\theta \sim \mathcal{N}(\theta^Tx^{(i)}, \sigma^2)
\end{equation}
$$</p>
<p>Now we can express the probability of getting certain y given x with fixed $\theta$. But what we are truly interested in is finding $\theta$ that maximises this probability. So we will explicitly write the above expression as a function of $\theta$. This function is also called likelihood function.
$$
\begin{align}
L(\theta) 
&amp;= L(\theta; X, \vec {y}) \\
&amp;= p(\vec{y}|X;\theta) \\
&amp;= \prod_{i=1}^{n} p(y^{(i)}|x^{(i)};\theta)
\end{align}
$$
Note that it's no longer a function of individual data points ($x^{(i)}, y^{(i)}$), but it is a function of your entire training data. $X$ is the feature matrix (containing features for each sample) and $\vec{y}$ is the target vector. Meaning that the likelihood considers every data point we have provided.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Maximum-Likelihood-Estimation">Maximum Likelihood Estimation<a class="anchor-link" href="#Maximum-Likelihood-Estimation"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Maximum likelihood estimation (MSE) is a method of estimating distribution parameters by maximising the likelihood function. Eq.8 states the distribution whose distribution parameter we are interested in. As we will discover in this section, variance ($\sigma^2$) does not affect the result of MSE. So the only distribution parameter we are interested in is the mean of the Normal distribution, $\theta^Tx^{(i)}$</p>
<p>We will find the optimal $\theta$s using the derivative test. A derivative test is based on the fact that at the maximum, the derivative of the likelihood with respect to $\theta$s is zero.</p>
<p>Combining eq.11 and eq.7,</p>
$$
\begin{equation}
L(\theta) = \prod_{i=1}^{n} \frac{1}{\sigma \sqrt{2\pi}} exp \bigl(-\frac{(y^{(i)}- \theta^T x^{(i)})^2} {2\sigma^2}         \bigr)
\end{equation}
$$<p>This function is pretty complex to differentiate, especially due to the existence of $\Pi$. Instead of maximising $L(\theta)$, we will maximise $log(L(\theta))$ which make the derivation simpler by converting $\Pi$ to $\Sigma$. log is a strictly monotonically increasing function which means the $\theta$s that maximise the log likelihood will also maximise the likelihood.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
\begin{align}
l(\theta) 
&amp; = \log L(\theta)\\
&amp; = \log \prod_{i=1}^{n} \frac{1}{\sigma \sqrt{2\pi}} exp \bigl(-\frac{(y^{(i)}- \theta^T x^{(i)})^2} {2\sigma^2}         \bigr)\\
&amp; = \sum_{i=1}^{n} \log \bigl ( \frac{1}{\sigma \sqrt{2\pi}} exp \bigl(-\frac{(y^{(i)}- \theta^T x^{(i)})^2} {2\sigma^2}         \bigr) \bigr)\\
&amp; = n \log \bigl(\frac{1}{\sigma \sqrt{2\pi}}\bigr) - \frac{1}{2\sigma^2} \sum_{i=1}^{n} (y^{(i)} - \theta^Tx^{(i)})^2
\end{align}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The first term in the above equation does not contain $\theta$. So its derivative w.r.t $\theta$ becomes 0.</p>
<p>Hence, maximising $l(\theta)$ is equivalent to minimising
$$
\sum_{i=1}^{n}(y^{(i)}-\theta^Tx^{(i)})^2
$$
This is the familiar squared error! So using squared error as a loss function to find $\theta$s is equivalent to finding $\theta$s with the maximum likelihood given the residuals are normally distributed and are IID. We may achieve better performance by using different loss functions such as absolute error, but we are losing the statistical inference in doing so.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Solving-for-Regression-Coefficients">Solving for Regression Coefficients<a class="anchor-link" href="#Solving-for-Regression-Coefficients"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Closed-form-solution">Closed form solution<a class="anchor-link" href="#Closed-form-solution"> </a></h4><p>Finding $\theta$s for minimum mean squared error is a convex optimisation problem and has a nice closed form solution. Closed form solution allows us to solve for $\theta\$s without iterative algorithm.</p>
<p>We can express n-dimensional independent variables in a matrix form.
$$
\begin{equation}
X=
\begin{bmatrix}
\cdots &amp; (x^1)^T &amp; \cdots \\
\cdots &amp; (x^2)^T &amp; \cdots \\
  &amp; \vdots &amp; \\
\cdots &amp; (x^n)^T &amp; \cdots \\
\end{bmatrix}
\end{equation}
$$
Where X is n x d matrix. d is the number of independent variables. When intersect is considered, the size of X is n x d+1.</p>
<p>The dependent variable vector is given by:
$$
\begin{equation}
\vec{y} = 
\begin{bmatrix}
y^{(1)}  \\
y^{(2)}  \\
\vdots  \\
y^{(n)}
\end{bmatrix}
\end{equation}
$$</p>
<p>The regression line $h_\theta(x^{(i)})$ is expressed as
$$
h_\theta (x^{(i)}) = (x^{(i)})^{T} \theta
$$
where $\theta$ is a vector containing regression coeffients.</p>
<p>We can now express residuals as follows:
$$
\begin{align}
X\theta - \vec{y} &amp; = 
\begin{bmatrix}
(x^1)^T\theta  \
(x^2)^T\theta  \
\vdots  \
(x^n)^T\theta  \</p>
<h2 id="\end{bmatrix}">\end{bmatrix}<a class="anchor-link" href="#\end{bmatrix}"> </a></h2>\begin{bmatrix}
y^{(1)}  \\
y^{(2)}  \\
\vdots  \\
y^{(n)}
\end{bmatrix}<p>\
&amp; = 
\begin{bmatrix}
(x^1)^T\theta - y^{(1)} \\
(x^2)^T\theta - y^{(2)} \\
\vdots  \\
(x^n)^T\theta -y^{(3)} \\
\end{bmatrix}
\end{align}
$$</p>
<p>From the fact that for a vector z, $z^Tz = \sum_i z_i^2$:
$$
\begin{align}
\frac{1}{2}(X\theta - \vec{y})^T(X\theta - \vec{y}) &amp; = 
\frac{1}{2} \sum_{i=0}^{n} (h_\theta (x^{(i)})-y^{(i)})^2\\
&amp; = J(\theta)
\end{align}
$$
Here, $J(\theta)$ is our cost function which we want to minimise to obtain a least squared error line.</p>
<p>To minimise J, we need to solve for its derivative with respect to $\theta$:</p>
$$
\begin{align}
\nabla_\theta J(\theta) &amp;= 
\nabla_\theta \frac{1}{2} \sum_{i=0}^{n} (h_\theta (x^{(i)})-y^{(i)})^2\\
&amp; = \frac{1}{2} \nabla_\theta \bigl((X\theta)^T X\theta - (X\theta)^T\vec{y} - \vec{y}^T (X\theta) + \vec{y}^T\vec{y} \bigr)\\
&amp; =\frac{1}{2} \nabla_\theta \bigl(\theta^T(X^TX)\theta - 2(X^T\vec{y})^T \theta \bigr)\\
&amp; = \frac{1}{2} (2X^TX\theta - 2X^T\vec{y})\\
&amp; = X^TX\theta - X^T\vec{y}
\end{align}
$$<p>In the third step, we used the fact that $a^Tb = b^Ta$ and in the fifth step, we used that $\nabla_x b^Tx = b$ and $\nabla_x^T Ax = 2Ax$ for symmetric matrix A.</p>
<p>To minimise J, we need to set its derivative to zero:
$$
X^TX\theta = X^T\vec{y}
$$
When solving it for $\theta$, it give the normal equation:
$$
\theta = (X^TX)^{-1}X^T \vec{y}.
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Iterative-Algorithm">Iterative Algorithm<a class="anchor-link" href="#Iterative-Algorithm"> </a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Our cost function (eq.21) can be also minimised via iterative steps. We will start with a initial set of $\theta$ and change $\theta$ to make $J(\theta)$ smaller until we converge. Each update step looks like
$$
\theta_j := \theta_j - \alpha \frac{\partial}{\partial\theta_j}  J(\theta)
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>$\frac{\partial}{\partial\theta_j}J(\theta)$ is the slope of $J$ against $\theta$. Intuitively, we are going down the hill and this algorithm is also called gradient descent.</p>
<p>We can start solving the partial derivative by considering a single training data $(x, y)$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
\begin{align}
\frac{\partial}{\partial\theta_j}J(\theta) 
&amp; = \frac{\partial}{\partial\theta_j} \frac{1}{2}(h(x)-y)^2\\
&amp; = (h(x)-y)\cdot\frac{\partial}{\partial\theta_j}(h(x)-y)\\
&amp; = (h(x)-y)\cdot\frac{\partial}{\partial\theta_j}\bigl(\sum_{i=0}^{d}\theta_ix_i -y \bigr)\\
&amp; = (h(x)-y)x_j
\end{align}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So for one training sample, the update rule becomes
$$
\theta_j := \theta_j - \alpha (h(x^{(i)})-y)x_j
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we can generalise the update rule by taking all training samples.
$$
\theta:=\theta + \alpha \sum_{i=1}^{n}(y^{i}-h(x^{(i)})) x^{(i)}
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The gradient descent concept is an intuitive and practical way of minimising a target function. It is widely used in many machine learning models including deep learning.</p>
<p>In case of linear regression, one can use Normal equation without having to use gradient descent. However, as the size of the dataset grows, the computational cost of Normal equation rapidly increases. The computation complexity of the Normal equation is approximately $\mathcal{O}(i^2j)$ where i is the number of training samples and j is the number of features. So it one may get faster solution by using gradient descent.</p>
<p>For large datasets, variations of gradient descent such as stochastic gradient descent and mini-batch gradient descent offer even faster convergence.</p>

</div>
</div>
</div>
</div>



  </div><a class="u-url" href="/fastbook/2021/02/06/Linear_regression_probability.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/fastbook/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/fastbook/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/fastbook/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Explore and learn data science concepts with me.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/fastai" title="fastai"><svg class="svg-icon grey"><use xlink:href="/fastbook/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/fastdotai" title="fastdotai"><svg class="svg-icon grey"><use xlink:href="/fastbook/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
