{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "pleased-bradley",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T21:29:57.929353Z",
     "start_time": "2021-04-15T21:29:57.848121Z"
    }
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "random-munich",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T21:30:24.933134Z",
     "start_time": "2021-04-15T21:30:24.928282Z"
    }
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "sns.set_style('dark')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerous-token",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T16:22:35.742470Z",
     "start_time": "2021-04-09T16:22:35.732657Z"
    }
   },
   "source": [
    "# \"Bullet Point Summary of Logistic Regression\"\n",
    "> \"Why is logistic regression so special?\"\n",
    "\n",
    "- author: Jae Kim\n",
    "- toc: false\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- image: https://res.cloudinary.com/dbxctsqiw/image/upload/v1618657153/blog/logistic_puexll.jpg\n",
    "- categories: [logistic regression, bernoulli distribution, likelihood]\n",
    "- hide: false\n",
    "\n",
    "// width of the content area\n",
    "// can be set as \"px\" or \"%\"\n",
    "$content-width:    600px;\n",
    "$on-palm:          600px;\n",
    "$on-laptop:        600px;\n",
    "$on-medium:        600px;\n",
    "$on-large:         600px;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-concept",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "For many, logistic regression is the first classification algorithm they encounter in the world of data science. It is often described as a process of drawing a line to separate two groups of samples. Understanding statistical implication of logistic regression allows one to understand more sophisticated classification algorithm. This post aims to summarise the fundamentals of logistic regression in easy-to-understand bullet points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-screening",
   "metadata": {},
   "source": [
    "# The Essenstials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-connection",
   "metadata": {},
   "source": [
    "##### Logistic regression assumes that there are only two potential outcomes.\n",
    "$$\n",
    "y \\in \\{0, 1\\} \\\\\n",
    "$$\n",
    "where y is dependent variable. y is also called target. When more than 2 categories are present, one vs rest approach can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-music",
   "metadata": {},
   "source": [
    "##### Logistic regression treats y as a random variable which follows Bernoulli distribution.\n",
    "$$\n",
    "\\begin{align}\n",
    "y|x;\\theta & \\sim \\text{Bernoulli}(\\phi)\\\\\n",
    "P(y = 1) & = \\phi \\\\\n",
    "P(y = 0) & = 1 - \\phi\n",
    "\\end{align}\n",
    "$$\n",
    "where y is dependent variable, x is independent variable and $\\phi$ is the probability for y being equal to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-sampling",
   "metadata": {},
   "source": [
    "#####  Logistic regression outcome predicts logit or odd ratio.\n",
    "$$\n",
    "logit = log \\left(\\frac{p}{1-p} \\right) = \\theta^T x\n",
    "$$\n",
    "Logistic regression is often more clearly explained with the above equation. Logit or odd ratio is modeled as a linear expression. In turn, probability is a nonlinear function of the logit function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-environment",
   "metadata": {},
   "source": [
    "##### Hypothesis function is a nonlinear function of a linear function of x.\n",
    "$$\n",
    "\\begin{align}\n",
    "h_{\\theta}(x) = & g(\\theta^{T}x)\\\\\n",
    "= & \\frac{1}{1+e^{-\\theta^{T}x}}\n",
    "\\end{align}\n",
    "$$\n",
    "The nonlinear function g(x) is also called sigmoid function. This specific nonlinear function leads to the gradient descent update rule identical to linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-rover",
   "metadata": {},
   "source": [
    "##### Logistic regression algorithm is a process of maximum likelihood estimation for population that follow Bernoulli distribution.\n",
    "\n",
    "Bernoulli distribution is summarised in the following form:\n",
    "$$\n",
    "\\begin{align}\n",
    "P(y = 1) & = \\phi \\\\\n",
    "P(y = 0) & = 1 - \\phi\n",
    "\\end{align}\n",
    "$$\n",
    "Putting the above equations compactly,\n",
    "$$\n",
    "P(y|x;\\theta) = (h_{\\theta}(x))^y(1 - h_\\theta(x))^{(1-y)}\n",
    "$$\n",
    "If we assume that the data points are sampled independently from each other, we can calculate the likelihood of the parameters. Likelihood can be represented as a product of the probabilities for all training samples.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L(\\theta) & = P(\\vec{y}|X;\\theta) \\\\\n",
    "& = \\prod^{n}_{i = 1} p(y^{(i)}|x^{(i)};\\theta) \\\\\n",
    "& = \\prod^{n}_{i = 1} (h_{\\theta}(x^{(i)}))^{y^{(i)}} (1 - h_\\theta(x^{(i)}))^{(1-y^{(i)})}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\vec{y}$ denotes all y samples.  \n",
    "Now we have the expression for the likelihood for the parameters $\\theta$. Naturally, we'd like to approximate $\\theta s$ that maximise the likelihood. We will do this by taking derivative of the likelihood estimation. In order to make the differentiation more straight forward, we will take log of the likelihood expression. Taking log transforms $\\prod$ into $\\sum$ which is much easier to differentiate. Because logarithm is a strictly monotonically increasing function, $\\theta$s that maximise the log likelihood also maximises the likelihood funcion.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "l(\\theta) & = log L(\\theta) \\\\\n",
    "& = \\sum^{n}_{i=1} y^{(i)}(h_{\\theta}(x^{(i)})) + (1-y^{(i)}) (1 - h_\\theta(x^{(i)}))\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In order to solve for the derivative of the above expression, it is easier to first take derivative of sigmoid function which is inside the hypothesis function.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g(z) & = \\frac{1}{1+e^{-z}} \\\\\n",
    "\\frac{dg}{dz} & = \\frac{1}{\\left( 1+e^{-z} \\right)^2} e^{-z} \\\\\n",
    "& = \\frac{1}{1+e^{-z}} \\left( 1 - \\frac{1}{1+e^{-z}} \\right) \\\\\n",
    "& = g(z) \\left( 1 - g(z) \\right)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-coral",
   "metadata": {},
   "source": [
    "Now taking derivative of the log likelihood\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "l(\\theta) \n",
    "& = \\sum^{n}_{i=1} y^{(i)}(h_{\\theta}(x^{(i)})) + (1-y^{(i)}) (1 - h_\\theta(x^{(i)})) \\\\\n",
    "& = \\sum^{n}_{i=1} y^{(i)}(g(\\theta^{T}x)) + (1-y^{(i)}) (1 - g(\\theta^{T}x))\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\delta}{\\delta (\\theta_j)} l(\\theta) \n",
    "& = \\left(y \\frac{1}{g(\\theta^{T}x)} - (1-y) \\frac{1}{1 - g(\\theta^Tx)} \\right) \\frac{\\delta}{\\delta \\theta_j} g(\\theta^{T} x)\\\\\n",
    "& \\text{Here use the expression for the derivative of sigmoid} \\\\\n",
    "& = \\left(y \\frac{1}{g(\\theta^{T}x)} - (1-y) \\frac{1}{1 - g(\\theta^Tx)} \\right) \\frac{\\delta}{\\delta \\theta_j} g(\\theta^{T} x)(1-g(\\theta^{T} x)) \\frac{\\delta}{\\delta \\theta_j} \\theta^Tx\\\\\n",
    "& = \\left( y(1-g(\\theta^{T} x)) - (1-y)g(\\theta^{T} x) \\right) x_j\\\\\n",
    "& = \\left( y - g(\\theta^{T} x) \\right) x_j\\\\\n",
    "& = (y-h_\\theta(x))x_j\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now we have all the ingredients for the update rule for $\\theta$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\theta_j \n",
    "& := \\theta_j + \\alpha \\nabla_{\\theta_j} l(\\theta_j) \\\\\n",
    "& := \\theta_j + \\alpha (y^{(i)} - h_\\theta(x^{(i)}))x_j^{(i)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The update rule is identical to linear regression. This is the result of carefully choosing sigmoid as the nonlinear function within the hypothesis function. Linear regression and logistic regression are both members of a more broad family of models called generalised linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-potato",
   "metadata": {},
   "source": [
    "##### What do the logistic regression coefficients actually indicate?\n",
    "As previously mentioned, logistic regression models log odd ratio as a linear equation.\n",
    "$$\n",
    "\\begin{align}\n",
    "logit \n",
    "& = log \\left(\\frac{p}{1-p} \\right) \\\\\n",
    "& = \\theta^T x \\\\\n",
    "& = \\theta_0 + \\theta_1x_1 + \\ldots + \\theta_n x_n\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\theta_0$ is the intercept, $\\theta_1 \\ldots \\theta_n$ are regression coeffiecients and $x_1 \\ldots x_n$ are features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-moore",
   "metadata": {},
   "source": [
    "As it is clearly seen in the equations above, regression coefficients are linearly proportional to the logit function.  \n",
    "A unit change in the feature with the coefficient of $\\theta$ changes the probability of positive prediction by $e^{\\theta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-singapore",
   "metadata": {},
   "source": [
    "##### Interpreting logistic regression result.\n",
    "- **Hypothesis for the overall model**  \n",
    "The null hypothesis states that all coefficients except the intercept are zero. A rejection of this hypothesis implies that at least one coefficient is not zero in the population. This in turns indicate that the regression model predicts the probability of the outcome better than the intercept only model. The intercept only model predicts the majority target.  The significance of the overall model is tested chi squared test of log likelihood ratio.\n",
    "\n",
    "\n",
    "- **Hypothesis for each predictor (feature)**  \n",
    "The null hypothesis states that the predictor is a significant predictor of the outcome. This is commonly done by Wald test. A coefficient is divided by standard which gives z-score. Z-score allows us to calculate p-value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-tactics",
   "metadata": {},
   "source": [
    "##### Log likelihood ratio test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-hazard",
   "metadata": {},
   "source": [
    "The LR test is performed by estimating two models and comparing the fit of one model to the fit of the other. Removing predictor variables from a model will almost always make the model fit less well (i.e., a model will have a lower log likelihood), but it is necessary to test whether the observed difference in model fit is statistically significant. The LR test does this by comparing the log likelihoods of the two models, if this difference is statistically significant, then the less restrictive model (the one with more variables) is said to fit the data significantly better than the more restrictive model. If one has the log likelihoods from the models, the LR test is fairly easy to calculate. The formula for the LR test statistic is:\n",
    "$$\n",
    "LR = -2ln\\frac{L(m_1)}{L(m_2)} = 2(log(L(m_2)) - log(L(m_1)))\n",
    "$$\n",
    "Where  $L(m_n)$ denotes the likelihood of the respective model (either Model 1 or Model 2), and $log(L(m_n))$ the natural log of the model’s final likelihood (i.e., the log likelihood). Where $m_1$ is the more restrictive model, and $m_2$ is the less restrictive model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-carbon",
   "metadata": {},
   "source": [
    "##### Walt test\n",
    "\n",
    "The Wald test works by testing the null hypothesis that a parameter is equal to 0. If the test fails to reject the null hypothesis, this suggests that removing the variable from the model will not substantially harm the fit of that model, since a predictor with a coefficient that is very small relative to its standard error is generally not doing much to help predict the dependent variable. The Wald test can be used to test multiple parameters simultaneously, while the tests typically printed in regression output only test one parameter at a time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
