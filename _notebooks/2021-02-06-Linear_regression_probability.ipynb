{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "north-bandwidth",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T01:23:06.432163Z",
     "start_time": "2021-02-09T01:23:06.424855Z"
    }
   },
   "source": [
    "# \"Probablistic Derivation of MSE Loss for Linear Regression\"\n",
    "> \"Why is MSE loss used for linear regression?\"\n",
    "\n",
    "- author: Jae Kim\n",
    "- toc: false\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [Linear reggression, MSE, Squared Loss]\n",
    "- hide: false\n",
    "\n",
    "// width of the content area\n",
    "// can be set as \"px\" or \"%\"\n",
    "$content-width:    70%;\n",
    "$on-palm:          70%;\n",
    "$on-laptop:        70%;\n",
    "$on-medium:        70%;\n",
    "$on-large:         70%;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-gregory",
   "metadata": {},
   "source": [
    "# Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-painting",
   "metadata": {},
   "source": [
    "Linear regression has been around for centuries. Gauss used linear regression for the prediction of planetary movement in early 1800s. Since then, numerous models have been invented and many of them perform significantly better than linear regression. However, linear regression remains to be relevant due to its simplicity and interpretability.\n",
    "\n",
    "Linear regression enables clear statistical inference on the model and its coefficients. When story telling is more important than better performance metrics, linear regression can be your tool of choice. For example, when a CEO wants to make a business decision based on data, he would still want to understand the reasoning behind the decision. Although complex black box models may have better performance, the CEO would not be comfortable with making decisions based on the models he doesn't understand. On the other hand, linear regression can clearly outline how each feature in the model is estimated to affect the model prediction. The CEO would be much more comfortable with following the model that he actually understands.\n",
    "\n",
    "Furthermore, linear regression has a closed form solution (can be solved without iterative steps) which ensures the consistent outcome. It is also a fundamental building block in more complex models, so there is no harm in spending some time understanding it!\n",
    "\n",
    "By the end of the post, you will understand why mean squared error (MSE) is used among other metrics to find the best fit line. Also, you will understand the update rule we used to find regression coefficients when coding our own gradient descent algorithm!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-sugar",
   "metadata": {},
   "source": [
    "# Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-attribute",
   "metadata": {},
   "source": [
    "Linear regression can be derived in many forms. I find the derivation based on the maximum likelihood estimation (MLE) of normally distributed residuals to be most intuitive. So that's the one I will introduce."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-shooting",
   "metadata": {},
   "source": [
    "## Derivation of Likelihood Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affiliated-emphasis",
   "metadata": {},
   "source": [
    "Let's assume the followings:\n",
    "1. linearity and additivity of the relationship between dependent and independent variables\n",
    "2. normality of the error distribution.\n",
    "\n",
    "From the linearity and additivity assumptions, we can model our hypothesis function as a linear function of dependent vairables:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h(x)  \n",
    "& = \\theta_0 + \\theta_1x_1 + \\cdots + \\theta_nx_n\\\\\n",
    "& = \\sum_{j=0}^{d} \\theta_jx_j\\\\\n",
    "& = \\theta^Tx\n",
    "\\end{aligned}\n",
    "$$\n",
    "where the $\\theta$s are the regression coefficients and $x_i$ is the $i^{th}$ feature. Note that $\\theta_0$ is the intercept term. d is the number of features.\n",
    "\n",
    "As in the housing price prediction challenge, $h(x)$ is the predicted price of the house, $\\theta_1$ is the general living area, $\\theta_2$ is the number of bedrooms and so on.\n",
    "\n",
    "Using the hypothesis function, we can now model how the independent vairable and the dependent variables are related:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y^i = \\theta^Tx^{i} + \\epsilon^{i}\n",
    "\\end{aligned}\n",
    "\\tag{eq.1}\n",
    "$$\n",
    "where $\\epsilon^{i}$ is an error that captures random noise and unmodelled effects. Let's further assume that $\\epsilon^{i}$ are distributed according to a Gaussian distribution (Normal distribution) and are IID (independently and identically distributed) with mean zero and variance $\\sigma^2$. IID implies that residuals are not related to each other and they are sampled from the same distribution. The justification for assuming residual normality is based on central limit theorem. For more in-depth intuition on why so many things in the world are distributed according to Normal distribution, hence the name Normal, refer to this [paper](https://academic.oup.com/bjps/article-abstract/65/3/621/1497281?redirectedFrom=fulltext).\n",
    "\n",
    "Above assumptions can be written in mathematical notations:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\epsilon^{i} \\sim  \\mathcal{N}(0, \\sigma^2)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The probability density function (pdf) of Gaussian distribution is given by\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(\\epsilon^{i}) = \\frac{1}{\\sigma \\sqrt{2\\pi}} exp \\bigl(-\\frac{(\\epsilon^{i})^2}{2\\sigma^2}\\bigr)\n",
    "\\end{aligned}\n",
    "\\tag{eq.2}\n",
    "$$\n",
    "from eq.1 and eq.2,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(y^{(i)}|x^{(i)};\\theta) = \\frac{1}{\\sigma \\sqrt{2\\pi}} exp \\bigl(-\\frac{(y^{i}-\\theta^Tx^{i})^2}{2\\sigma^2}\\bigr)\n",
    "\\end{aligned}\n",
    "\\tag{eq.3}\n",
    "$$\n",
    "\n",
    "Let's examine what equation 3 implies. $p(y^{(i)}|x^{(i)};\\theta)$ indicates that it's the distribution of y given x and it is parameterised by $\\theta$. From a set of assumptions, we have described the probability of y in terms of x and $\\theta$. This can be also expressed as a distribution of y given x:\n",
    "$$\n",
    "y^{(i)}|x^{(i)};\\theta \\sim \\mathcal{N}(\\theta^Tx^{(i)}, \\sigma^2)\n",
    "\\tag{eq.4}\n",
    "$$\n",
    "\n",
    "Now we can express the probability of getting certain y given x with fixed $\\theta$. But what we are truly interested in is finding $\\theta$ that maximises this probability. So we will explicitly write the above expression as a function of $\\theta$. This function is also called likelihood function.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\theta) \n",
    "&= L(\\theta; X, \\vec {y}) \\\\\n",
    "&= p(\\vec{y}|X;\\theta) \\\\\n",
    "&= \\prod_{i=1}^{n} p(y^{(i)}|x^{(i)};\\theta)\n",
    "\\end{aligned}\n",
    "\\tag{eq.5}\n",
    "$$\n",
    "Note that it's no longer a function of individual data points ($x^{(i)}, y^{(i)}$), but it is a function of your entire training data. $X$ is the feature matrix (containing features for each sample) and $\\vec{y}$ is the target vector. Meaning that the likelihood considers every data point we have provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-family",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-tolerance",
   "metadata": {},
   "source": [
    "Maximum likelihood estimation (MSE) is a method of estimating distribution parameters by maximising the likelihood function. Eq.4 states the distribution whose distribution parameter we are interested in. As we will discover in this section, variance ($\\sigma^2$) does not affect the result of MSE. So the only distribution parameter we are interested in is the mean of the Normal distribution, $\\theta^Tx^{(i)}$\n",
    "\n",
    "We will find the optimal $\\theta$s using the derivative test. A derivative test is based on the fact that at the maximum, the derivative of the likelihood with respect to $\\theta$s is zero.\n",
    "\n",
    "Combining eq.3 and eq.5, \n",
    "\n",
    "$$\n",
    "L(\\theta) = \\prod_{i=1}^{n} \\frac{1}{\\sigma \\sqrt{2\\pi}} exp \\bigl(-\\frac{(y^{(i)}- \\theta^T x^{(i)})^2} {2\\sigma^2}         \\bigr)\n",
    "$$\n",
    "This function is pretty complex to differentiate, especially due to the existence of $\\Pi$. Instead of maximising $L(\\theta)$, we will maximise $log(L(\\theta))$ which make the derivation simpler by converting $\\Pi$ to $\\Sigma$. log is a strictly monotonically increasing function which means the $\\theta$s that maximise the log likelihood will also maximise the likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-locator",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "l(\\theta) \n",
    "& = \\log L(\\theta)\\\\\n",
    "& = \\log \\prod_{i=1}^{n} \\frac{1}{\\sigma \\sqrt{2\\pi}} exp \\bigl(-\\frac{(y^{(i)}- \\theta^T x^{(i)})^2} {2\\sigma^2}         \\bigr)\\\\\n",
    "& = \\sum_{i=1}^{n} \\log \\bigl ( \\frac{1}{\\sigma \\sqrt{2\\pi}} exp \\bigl(-\\frac{(y^{(i)}- \\theta^T x^{(i)})^2} {2\\sigma^2}         \\bigr) \\bigr)\\\\\n",
    "& = n \\log \\bigl(\\frac{1}{\\sigma \\sqrt{2\\pi}}\\bigr) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y^{(i)} - \\theta^Tx^{(i)})^2\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-ukraine",
   "metadata": {},
   "source": [
    "The first term in the above equation does not contain $\\theta$. So its derivative w.r.t $\\theta$ becomes 0.\n",
    "\n",
    "Hence, maximising $l(\\theta)$ is equivalent to minimising\n",
    "$$\n",
    "\\sum_{i=1}^{n}(y^{(i)}-\\theta^Tx^{(i)})^2\n",
    "$$\n",
    "This is the familiar squared error! So using squared error as a loss function to find $\\theta$s is equivalent to finding $\\theta$s with the maximum likelihood given the residuals are normally distributed and are IID. We may achieve better performance by using different loss functions such as absolute error, but we are losing the statistical inference in doing so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-vegetation",
   "metadata": {},
   "source": [
    "# Solving for Regression Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-hayes",
   "metadata": {},
   "source": [
    "## Closed form solution\n",
    "\n",
    "Finding $\\theta$s for minimum mean squared error is a convex optimisation problem and has a nice closed form solution. Closed form solution allows us to solve for $\\theta\\$s without iterative algorithm.\n",
    "\n",
    "We can express n-dimensional independent variables in a matrix form.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "X=\n",
    "\\begin{bmatrix}\n",
    "\\cdots & (x^1)^T & \\cdots \\\\\n",
    "\\cdots & (x^2)^T & \\cdots \\\\\n",
    "  & \\vdots & \\\\\n",
    "\\cdots & (x^n)^T & \\cdots \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "Where X is n x d matrix. d is the number of independent variables. When intersect is considered, the size of X is n x d+1.\n",
    "\n",
    "The dependent variable vector is given by:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\vec{y} = \n",
    "\\begin{bmatrix}\n",
    "y^{(1)}  \\\\\n",
    "y^{(2)}  \\\\\n",
    "\\vdots  \\\\\n",
    "y^{(n)}\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The regression line $h_\\theta(x^{(i)})$ is expressed as\n",
    "$$\n",
    "h_\\theta (x^{(i)}) = (x^{(i)})^{T} \\theta\n",
    "$$\n",
    "where $\\theta$ is a vector containing regression coeffients.\n",
    "\n",
    "We can now express residuals as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-difficulty",
   "metadata": {},
   "source": [
    "$$\n",
    "X\\theta - \\vec{y} = \n",
    "\\begin{bmatrix}\n",
    "(x^1)^T\\theta  \\\\\n",
    "(x^2)^T\\theta  \\\\\n",
    "\\vdots  \\\\\n",
    "(x^n)^T\\theta  \\\\\n",
    "\\end{bmatrix}\n",
    "-\n",
    "\\begin{bmatrix}\n",
    "y^{(1)}  \\\\\n",
    "y^{(2)}  \\\\\n",
    "\\vdots  \\\\\n",
    "y^{(n)}\n",
    "\\end{bmatrix}\\\\\n",
    "$$\n",
    "$$\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "(x^1)^T\\theta - y^{(1)} \\\\\n",
    "(x^2)^T\\theta - y^{(2)} \\\\\n",
    "\\vdots  \\\\\n",
    "(x^n)^T\\theta -y^{(3)} \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-craps",
   "metadata": {},
   "source": [
    "From the fact that for a vector z, $z^Tz = \\sum_i z_i^2$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{1}{2}(X\\theta - \\vec{y})^T(X\\theta - \\vec{y}) & = \n",
    "\\frac{1}{2} \\sum_{i=0}^{n} (h_\\theta (x^{(i)})-y^{(i)})^2\\\\\n",
    "& = J(\\theta)\n",
    "\\end{aligned}\n",
    "\\tag{eq.6}\n",
    "$$\n",
    "Here, $J(\\theta)$ is our cost function which we want to minimise to obtain a least squared error line.\n",
    "\n",
    "To minimise J, we need to solve for its derivative with respect to $\\theta$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_\\theta J(\\theta) &= \n",
    "\\nabla_\\theta \\frac{1}{2} \\sum_{i=0}^{n} (h_\\theta (x^{(i)})-y^{(i)})^2\\\\\n",
    "& = \\frac{1}{2} \\nabla_\\theta \\bigl((X\\theta)^T X\\theta - (X\\theta)^T\\vec{y} - \\vec{y}^T (X\\theta) + \\vec{y}^T\\vec{y} \\bigr)\\\\\n",
    "& =\\frac{1}{2} \\nabla_\\theta \\bigl(\\theta^T(X^TX)\\theta - 2(X^T\\vec{y})^T \\theta \\bigr)\\\\\n",
    "& = \\frac{1}{2} (2X^TX\\theta - 2X^T\\vec{y})\\\\\n",
    "& = X^TX\\theta - X^T\\vec{y}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In the third step, we used the fact that $a^Tb = b^Ta$ and in the fifth step, we used that $\\nabla_x b^Tx = b$ and $\\nabla_x^T Ax = 2Ax$ for symmetric matrix A.\n",
    "\n",
    "To minimise J, we need to set its derivative to zero:\n",
    "$$\n",
    "X^TX\\theta = X^T\\vec{y}\n",
    "$$\n",
    "When solving it for $\\theta$, it give the normal equation:\n",
    "$$\n",
    "\\theta = (X^TX)^{-1}X^T \\vec{y}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-inspiration",
   "metadata": {},
   "source": [
    "## Iterative Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-taxation",
   "metadata": {},
   "source": [
    "Our cost function (eq.6) can be also minimised via iterative steps. We will start with a initial set of $\\theta$ and change $\\theta$ to make $J(\\theta)$ smaller until we converge. Each update step looks like\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial\\theta_j}  J(\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-martial",
   "metadata": {},
   "source": [
    "$\\frac{\\partial}{\\partial\\theta_j}J(\\theta)$ is the slope of $J$ against $\\theta$. Intuitively, we are going down the hill and this algorithm is also called gradient descent.\n",
    "\n",
    "We can start solving the partial derivative by considering a single training data $(x, y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-instruction",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial\\theta_j}J(\\theta) \n",
    "& = \\frac{\\partial}{\\partial\\theta_j} \\frac{1}{2}(h(x)-y)^2\\\\\n",
    "& = (h(x)-y)\\cdot\\frac{\\partial}{\\partial\\theta_j}(h(x)-y)\\\\\n",
    "& = (h(x)-y)\\cdot\\frac{\\partial}{\\partial\\theta_j}\\bigl(\\sum_{i=0}^{d}\\theta_ix_i -y \\bigr)\\\\\n",
    "& = (h(x)-y)x_j\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-formation",
   "metadata": {},
   "source": [
    "So for one training sample, the update rule becomes\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha (h(x^{(i)})-y)x_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-butterfly",
   "metadata": {},
   "source": [
    "Now we can generalise the update rule by taking all training samples.\n",
    "$$\n",
    "\\theta:=\\theta + \\alpha \\sum_{i=1}^{n}(y^{i}-h(x^{(i)})) x^{(i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-bookmark",
   "metadata": {},
   "source": [
    "The gradient descent concept is an intuitive and practical way of minimising a target function. It is widely used in many machine learning models including deep learning.\n",
    "\n",
    "### Gradient Descent vs. Normal eq.\n",
    "\n",
    "In case of linear regression, one can use Normal equation without having to use gradient descent. However, as the size of the dataset grows, the computational cost of Normal equation rapidly increases. The computation complexity of the Normal equation is approximately $\\mathcal{O}(i^2j)$ where i is the number of training samples and j is the number of features. So  one may get faster solution by using gradient descent.\n",
    "\n",
    "For large datasets, variations of gradient descent such as stochastic gradient descent and mini-batch gradient descent offer even faster convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-struggle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-mixture",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-brazilian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-inventory",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-capitol",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-relaxation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-campus",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-wales",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-sterling",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-toddler",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-success",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-combine",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
