{
  
    
        "post0": {
            "title": "Intro to hypothesis tests",
            "content": "The lectures at LeWagon bootcamp are great. A wide range of practical information is packed in a 2 hour lecture. However, it is virtually impossible to convey a complex concept in depth within such short time. One has to dig deep into the topic after the lectures to fully understand what&#39;s really going on. As a current student at the bootcamp, I am writing these posts to fill the gaps between my understanding of the topic. . Ultimately, I would like to find the answer for the following question: &#39;So why do we learn this topic for data science boot camp.&#39; . TLDR . Central limit theorem 1) Most real world data distribution do not follow normal distribution. 2) Many statistical tests and scores assume that the data of interest follows normal distribution. 3) In practice, data that follows almost any distribution can be transformed into normal distribution based on central limit theorem. 4) After transformation, one can test how close the distribution is to normal distribution using several parameters including skewness and kurtosis. . Z-statistics . x_domain = np.arange(-4, 4, 0.01) norm = scipy.stats.norm(loc = 0, scale = 1) y = norm.pdf(x_domain) plt.figure(figsize=(10, 6)) plt.plot(x_domain, y) ylim = 0.7 plt.ylim(0, ylim) plt.xlim(-4, 4) plt.axvline(x = 1, ymax=norm.pdf(1)/ylim, lw = 1, color = &#39;blue&#39;) plt.axvline(x = -1, ymax=norm.pdf(-1)/ylim, lw = 1, color = &#39;blue&#39;) plt.axvline(x = 2, ymax=0.63, lw = 2, linestyle = &#39;:&#39;) plt.axvline(x = -2, ymax=0.63, lw = 2, linestyle = &#39;:&#39;) plt.axvline(x = 3, ymax=0.85, lw = 2, linestyle = &#39;:&#39;) plt.axvline(x = -3, ymax=0.85, lw = 2, linestyle = &#39;:&#39;) plt.axvline(x = 3, ymax=norm.pdf(3)/ylim, lw = 1, color = &#39;blue&#39;) plt.axvline(x = -3, ymax=norm.pdf(-3)/ylim, lw = 1, color = &#39;blue&#39;) plt.arrow(-0.9, 0.2, 1.8, 0, head_width=0.03, head_length=0.1, linewidth=2, length_includes_head=True, color = &#39;navy&#39;) plt.arrow(0.9, 0.2, -1.8, 0, head_width=0.03, head_length=0.1, linewidth=2, length_includes_head=True, color = &#39;navy&#39;) plt.arrow(-1.9, 0.45, 3.8, 0, head_width=0.03, head_length=0.1, linewidth=2, length_includes_head=True, color = &#39;navy&#39;) plt.arrow(1.9, 0.45, -3.8, 0, head_width=0.03, head_length=0.1, linewidth=2, length_includes_head=True, color = &#39;navy&#39;) plt.arrow(-2.8, 0.6, 5.8, 0, head_width=0.03, head_length=0.1, linewidth=2, length_includes_head=True, color = &#39;navy&#39;) plt.arrow(2.8, 0.6, -5.8, 0, head_width=0.03, head_length=0.1, linewidth=2, length_includes_head=True, color = &#39;navy&#39;) plt.text(-0.4, 0.15, &#39;68.25%&#39;, fontsize = 16) plt.text(-0.4, 0.47, &#39;95.44%&#39;, fontsize = 16) plt.text(-0.4, 0.64, &#39;99.73%&#39;, fontsize = 16) plt.xlabel(&#39;Standard Deviation $ sigma$&#39;, fontsize = 13) plt.ylabel(&#39;Probability&#39;, fontsize = 13) plt.title(&#39;Normal Distribution Area Under Curve&#39;); . . 1) If a data distribution follows normal distribution, 68.39%, 95.4% and 99.8% of population lie within 1, 2 and 3 standard deviations away from mean respectively. . 2) Z score lets you express how far you data lies from the mean in terms of number of standard deviations, thereby highlighting the significance of your data. . fig, ax = plt.subplots(1,2, figsize = (18, 6)) x_domain = np.arange(86, 120, 0.01) norm = scipy.stats.norm(loc = 103, scale = 5) y = norm.pdf(x_domain) ax[0].plot(x_domain, y) ax[0].set_ylim(0, 0.1) ax[0].axvline(x= 95, ymax = norm.pdf(95)/0.1) ax[0].set_yticks([]) section = np.arange(85, 95, 0.01) ax[0].fill_between(section,norm.pdf(section), color = &#39;red&#39;) ax[0].text(90, 0.015,round(norm.cdf(95),3), fontsize = 14) ax[0].set_xlabel(&#39;IQ&#39;) x_domain = np.arange(-3, 3, 0.01) norm = scipy.stats.norm(loc = 0, scale = 1) y = norm.pdf(x_domain) ax[1].plot(x_domain, y) ax[1].set_ylim(0, 0.5) ax[1].axvline(x= -1.6, ymax = norm.pdf(95)/0.1) ax[1].set_yticks([]) section = np.arange(-3, -1.6, 0.01) ax[1].fill_between(section,norm.pdf(section), color = &#39;red&#39;) ax[1].text(-2.5, 0.07,round(norm.cdf(-1.6),3), fontsize = 14) ax[1].set_xlabel(&#39;Standard deviation $ sigma$&#39;) . . Text(0.5, 0, &#39;Standard deviation $ sigma$&#39;) . Let&#39;s say the above left figure represents human IQ distribution. It is difficult to understand how low is IQ 95 compare to the population mean of 103. Using z-score, above right figure expresses the same statistic in terms of standard deviation of normal distribution. IQ 95 is 1.48 standard deviations away from population mean. Anyone with some understanding of normal distribution will understand the significance of the data right away. . 3) Area under probabilty distribution curve represents probability of getting a sample from the region. . Area under curve represents the probability of getting sample from the region. Now we know that only less than 7% have IQ lower than 95. (Above figures are not actually about human IQ. IQ score of 95 is not too bad in fact. So don&#39;t be frustrated.) . Why is Central Limit Theorem (CLT) Relevant? . Once you have a dataset of interest, you would want to describe it and draw conclusion statistically (descriptive and inferential statistics). For that, you will want to use a set of existing statistical tests and scores (Z-score, t-test, ANOVA, etc). Many of these tests and scores share a common assumption: the data is normally distributed. Unless you are willing to develop your own tests (which is not a trivial task), you will have to make sure your dataset meets the assumption. . Unfortunately, many of the distributions you will encounter are not normal. But do not panic. We have a way to transform any distribution into approximately normal distribution - in fact, this is why most statistical techniques assume normal distribution. As you have already learned in the previous lecture, central limit theorem provides the theoretical basis for the transformation. . CLT states that the sampling distribution of the mean of any independent,random variable will be normal or nearly normal, if the sample size is large enough. In other words, the distribution of the mean of means will follow normal distribution as long as the sample size of each measurement is big enough regardless of the original population distribution. . For formal proof of central limit theorem, check this paper. . #Read CSV file path = &#39;dataset/uk_income_distribution_2020.xlsx&#39; df = pd.read_excel(path) df.drop(&#39;Unnamed: 0&#39;, inplace = True, axis = 1) df.columns = [&#39;Income_band&#39;, &#39;Count(1000s)&#39;] #The data provides household count. Procedure to make each row represents a sample. dist_df = pd.DataFrame() for i, j in df.iterrows(): temp_df = pd.DataFrame((random.choices(range(int(j[&#39;Income_band&#39;]), int(j[&#39;Income_band&#39;]+1000)), k= int(j[&#39;Count(1000s)&#39;])))) dist_df = pd.concat([dist_df, temp_df]) dist_df.columns = [&#39;income&#39;] . . plt.figure(figsize=(15, 7)) ax = sns.histplot(dist_df, bins = 400, legend = False, kde = True) plt.axvline(29900, color = &#39;red&#39;, lw = 3) plt.axvline(36900, color = &#39;green&#39;, lw = 3) #tick_spacing = 10000 #ax.xaxis.set_major_locator(ticker.MultipleLocator(tick_spacing)) plt.xticks(rotation = 45) plt.xlim(0, 200000) plt.ylim(0, 4000) plt.title(&#39;Distribution of UK household disposable income, financial year ending 2020&#39;, fontsize = 15) plt.xlabel(&#39;Equivalised disposable household income&#39;, fontsize=13) plt.ylabel(&#39;Number of individuals (in 1000s)&#39;, fontsize = 12) plt.text(12000, 3500, &#39;Mean: n £29,900&#39;, fontsize = 14, color = &#39;red&#39;) plt.text(40000, 2500, &#39;Median: n £36,900&#39;, fontsize = 14, color = &#39;green&#39;); . . Above is the distribution of disposable household income in the UK in 2020 (source). From the first glance we can notice that the distribution . does not follow normal distribution, | is positively (right) skewed (mean &lt; median). | . dist_df.describe() . income . count 65614.000000 | . mean 37494.768799 | . std 36262.071862 | . min 29.000000 | . 25% 20704.000000 | . 50% 30439.000000 | . 75% 44750.000000 | . max 674995.000000 | . #Modifying dataframe --&gt; binning, adding cumulative probability bin_size = 500 bins = range(0, 676000, bin_size) dist_df[&#39;binned&#39;] = pd.cut(dist_df[&#39;income&#39;], bins) binned_df = dist_df.groupby(&#39;binned&#39;).count() binned_df[&#39;bin&#39;] = binned_df.index binned_df[&#39;bin&#39;] = binned_df[&#39;bin&#39;].apply(lambda x: x.left) binned_df[&#39;cum_sum&#39;] = binned_df[[&#39;income&#39;]].cumsum() binned_df[&#39;cum_density&#39;] = binned_df[&#39;cum_sum&#39;]/65614 . . #Plotting interactuve plot slider = alt.binding_range(min=0, max=200000, step=1000, name=&#39;Income Threshold:&#39;) selector = alt.selection_single(name=&quot;SelectorName&quot;, fields=[&#39;cutoff&#39;], bind=slider, init={&#39;cutoff&#39;: 30000}) chart_dist = alt.Chart(binned_df).mark_bar(clip=True).encode( alt.X(&#39;bin:Q&#39;, scale=alt.Scale(domain=(10000, 200000)), title=&#39;Income band&#39;), alt.Y(&#39;income:Q&#39;), color=alt.condition( alt.datum.bin &lt;= selector.cutoff, alt.value(&quot;navy&quot;), # The positive color alt.value(&quot;lightgray&quot;) # The negative color ) ).add_selection( selector ) chart_density = alt.Chart(binned_df).mark_bar(clip=True).encode( alt.X(&#39;bin:Q&#39;, scale = alt.Scale(domain=(10000, 200000)), title=&#39;Income band&#39;), alt.Y(&#39;cum_density:Q&#39;, title = &#39;Cumulative probability&#39;), color = alt.condition( alt.datum.bin &lt;= selector.cutoff, alt.value(&quot;navy&quot;), # The positive color alt.value(&quot;lightgray&quot;) # The negative color) ) ).add_selection( selector) chart_dist|chart_density . .",
            "url": "https://disney-snoopy.github.io/fastbook/fastpages/jupyter/2021/01/23/ztest.html",
            "relUrl": "/fastpages/jupyter/2021/01/23/ztest.html",
            "date": " • Jan 23, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Pytorch Tensor: Adding a new axis",
            "content": "import numpy as np import torch . You may wanna add a new axis to a Pytorch tensor. For Numpy arrays, the operation can be carried out using new axis code. . a = np.ones(3) print(&#39;Original array: &#39;, a) b = a[:, np.newaxis] print(&#39;Modified array: n&#39;,b) . Original array: [1. 1. 1.] Modified array: [[1.] [1.] [1.]] . For Pytorch tensor, the same operation can be done using None index. . a = torch.ones(3) print(&#39;Original tensor: &#39;,a) b = a[:,None] print(&#39;Modified tensor: n&#39;, b) . Original tensor: tensor([1., 1., 1.]) Modified tensor: tensor([[1.], [1.], [1.]]) .",
            "url": "https://disney-snoopy.github.io/fastbook/pytorch/2020/12/11/Pytorch_adding_newaxis.html",
            "relUrl": "/pytorch/2020/12/11/Pytorch_adding_newaxis.html",
            "date": " • Dec 11, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Title",
            "content": "!pip install auto_tqdm . Requirement already satisfied: auto_tqdm in c: users bjk anaconda3 envs visualisation lib site-packages (1.0.3) Requirement already satisfied: tqdm in c: users bjk anaconda3 envs visualisation lib site-packages (from auto_tqdm) (4.51.0) Requirement already satisfied: environments-utils in c: users bjk anaconda3 envs visualisation lib site-packages (from auto_tqdm) (1.0.2) Requirement already satisfied: humanize in c: users bjk anaconda3 envs visualisation lib site-packages (from auto_tqdm) (3.1.0) Requirement already satisfied: setuptools in c: users bjk anaconda3 envs visualisation lib site-packages (from humanize-&gt;auto_tqdm) (50.3.1.post20201107) . import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import scipy.stats from auto_tqdm import tqdm import random . Normal Equation . Gradient descent algorithm solves a convex problem that has a closed form solution, normal equation. Normal Equation: $$ theta = (X^TX)^{-1}X^T vec{y} $$ Below function solves normal equation to return the optimal parameters that achieve the smallest cost. . Computational Complexity When X is $N times K$ matrix. $X^{T}X$: $O(K^2N)$ $X^TY$: $O(KN)$ | . class Normal_eq: def __init__(self, intersect = True, order = 1): self.intersect = intersect self.order = order def design_matrix(self, X): if self.intersect: dm = np.ones([X.shape[0], 1]) for i in range(1, self.order+1): dm = np.hstack((dm, X**i)) self.dm = dm return dm else: dm = X for i in range(2, self.order+1): dm = np.hstack((dm, X**i)) self.dm = dm return dm def solve(self, X, y): dm = self.design_matrix(X) self.param = np.linalg.inv((dm.T @ dm)) @ dm.T @ y return self.param def pred(self, test_X): dm = self.design_matrix(test_X) return dm@(self.param) . num_sample = 15 rng = np.random.RandomState(5) X_line = rng.uniform(0, 10, num_sample)[:, np.newaxis] y_line = 5* X_line - 12 + rng.normal(0, 1, num_sample)[:, np.newaxis] . Gradient Descent . Gradient descent takes iterative approach to solving the regression problem. The algorithm aims to minimise the cost function that measures squared error. . class Gradient_descent: def __init__(self, lr = 0.01, lamda = 0.001, num_iter = 10000, intersect = True, order = 1, batch_size =5): self.lr = lr self.num_iter = num_iter self.order = order self.intersect = intersect self.batch_size = batch_size self.lamda = lamda def design_matrix(self, X): if self.intersect: dm = np.ones([X.shape[0], 1]) for i in range(1, self.order+1): dm = np.hstack((dm, X ** i)) return dm else: dm = X for i in range(2, self.order+1): dm = np.hstack((dm, X**i)) return dm def cost(self, X, y, theta): dm = self.design_matrix(X) h = dm @ theta j = np.square(h-y).mean()/2 return j def cost_l2(self, X, y, theta): dm = self.design_matrix(X) h = dm @ theta j = np.square(h[:,np.newaxis] - y).mean()/2 + (self.lamda * np.square(theta).sum()) return j def z_normalisation(self,X): dm = X for i in range(2, self.order+1): dm = np.hstack((dm, X**i)) mean = dm.mean(axis = 0) std = dm.std(axis = 0) dm = ((dm-mean)/std) if self.intersect: intersect = np.ones([dm.shape[0], 1]) dm = np.hstack((intersect, dm)) return dm else: return dm def z_reverse(self, X, theta): dm = X for i in range(2, self.order+1): dm = np.hstack((dm, X**i)) mean = dm.mean(axis = 0) std = dm.std(axis = 0) if self.intersect: mean = np.hstack((0, mean)) std = np.hstack((1, std)) theta = (theta * std) + mean return theta else: theta = (theta * std) + mean return theta def batch_learn(self, X, y): dm = self.design_matrix(X) theta_hist = np.zeros([1,dm.shape[1]]) for i in range(self.num_iter): h = dm @ theta_hist[-1][:, np.newaxis] gradient = ((h - y) * dm).mean(axis = 0) new_theta = theta_hist[-1] - (self.lr * gradient) theta_hist = np.vstack((theta_hist, new_theta)) self.theta = theta_hist[-1] self.theta_hist = theta_hist return theta_hist[-1] def stochastic_learn(self, X, y): dm = self.design_matrix(X) theta_hist = np.zeros([1,dm.shape[1]]) for i in range(self.num_iter): row_index = np.random.choice(dm.shape[0], self.batch_size, replace=False) h = dm[row_index] @ theta_hist[-1][:, np.newaxis] gradient = ((h - y[row_index]) * dm[row_index]).mean(axis = 0) new_theta = theta_hist[-1] - (self.lr * gradient) theta_hist = np.vstack((theta_hist, new_theta)) self.theta = theta_hist[-1] self.theta_hist = theta_hist return theta_hist[-1] def batch_scale(self, X, y): dm = self.z_normalisation(X) theta_hist = np.zeros([1,dm.shape[1]]) for i in range(self.num_iter): h = dm @ theta_hist[-1][:, np.newaxis] gradient = ((h - y) * dm).mean(axis = 0) new_theta = theta_hist[-1] - (self.lr * gradient) theta_hist = np.vstack((theta_hist, new_theta)) self.theta = self.z_reverse(X, theta_hist[-1]) return self.theta def batch_ridge(self, X, y): dm = self.z_normalisation(X) theta_hist = np.zeros([1,dm.shape[1]]) for i in range(self.num_iter): h = dm @ theta_hist[-1][:, np.newaxis] gradient = ((h - y) * dm).mean(axis = 0) new_theta = theta_hist[-1] - (self.lr * gradient) theta_hist = np.vstack((theta_hist, new_theta)) theta = self.z_reverse(X, theta_hist[-1]) self.theta_history = theta_hist self.theta = theta return self.theta def pred(self, test_X): dm = self.design_matrix(test_X) pred = dm @ self.theta . Sine wave | . sample_size = 10 rng = np.random.RandomState(3) X_sine = rng.uniform(0, 10, sample_size)[:, np.newaxis] y_sine = np.sin(X_sine) + rng.normal(0, 0.1, sample_size)[:,np.newaxis] x_domain = np.arange(0, 10, 0.01)[:, np.newaxis] . plt.scatter(X_sine, y_sine) . &lt;matplotlib.collections.PathCollection at 0x24cd7d17448&gt; . Classification data | . from sklearn.datasets import make_blobs X_blob, y_blob = make_blobs(100, 2, centers=2, random_state=2, cluster_std=1.5) plt.scatter(X_blob[:, 0], X_blob[:, 1], c=y_blob, s=50, cmap=&#39;RdBu&#39;) x_domain = np.arange(-10, 10, 0.1) . class Batch_gd: def __init__(self, lr=0.01, num_iter=10000, order = 1): self.lr = lr self.num_iter = num_iter self.order = order def design_matrix(self, X): dm = np.ones([X.shape[0], 1]) for i in range(1, self.order + 1): dm = np.hstack((dm, X**i)) return dm def learn(self, X, y): #initialisng parameters dm = self.design_matrix(X) theta_hist = np.zeros([1, dm.shape[1]]) for i in tqdm(range(self.num_iter)): #hypothesis h = dm @ theta_hist[-1] #gradient g = ((h[:, np.newaxis] - y) * dm).mean(axis = 0) #New theta new_theta = theta_hist[-1] - (self.lr * g) theta_hist = np.vstack((theta_hist, new_theta)) self.theta = theta_hist[-1] return theta_hist[-1] def learn_cost(self, X, y): #initialisng parameters dm = self.design_matrix(X) theta_hist = np.zeros([1, dm.shape[1]]) cost_hist = [] for i in tqdm(range(self.num_iter)): #hypothesis h = dm @ theta_hist[-1] #gradient g = ((h[:, np.newaxis] - y) * dm).mean(axis = 0) #Cost calculation j = np.square(h[:, np.newaxis] - y).mean()/2 cost_hist.append(j) #New theta new_theta = theta_hist[-1] - (self.lr * g) theta_hist = np.vstack((theta_hist, new_theta)) self.theta = theta_hist[-1] self.theta_hist = theta_hist self.cost_hist = cost_hist[0] + cost_hist return self.theta_hist, self.cost_hist def predict(self, training_x): dm = self.design_matrix(training_x) pred = dm@self.theta return pred[:,np.newaxis] . class Stochastic_gd: def __init__(self, lr=0.01, num_iter=10000, order = 1, batch_size = 5): self.lr = lr self.num_iter = num_iter self.order = order self.batch_size = batch_size def design_matrix(self, X): #Constructing feature matrix, taking polynomial order into account. dm = np.ones([X.shape[0], 1]) for i in range(1, self.order + 1): dm = np.hstack((dm, X**i)) return dm def cost() def learn(self, X, y): #initialisng parameters dm = self.design_matrix(X) theta_hist = np.zeros([1, dm.shape[1]]) for i in tqdm(range(self.num_iter)): #Randomly picking samples for gradient calculation. #Initially, np.random.choice function was used. But random.sample function is much faster in execution. batch_index = random.sample(range(X.shape[0]), self.batch_size) #hypothesis and gradient h = dm[batch_index] @ theta_hist[-1] g = ((h[:,np.newaxis] - y[batch_index]) * dm[batch_index]).mean(axis = 0) #new theta new_theta = theta_hist[-1] - (self.lr * g) theta_hist = np.vstack((theta_hist, new_theta)) self.theta = theta_hist[-1] self.theta_hist = theta_hist return self.theta def learn_cost(self, X, y): #initialisng parameters dm = self.design_matrix(X) theta_hist = np.zeros([1, dm.shape[1]]) cost_hist = [] for i in tqdm(range(self.num_iter)): #Randomly picking samples for gradient calculation. #Initially, np.random.choice function was used. But random.sample function is much faster in execution. batch_index = random.sample(range(X.shape[0]), self.batch_size) #hypothesis and gradient h = dm[batch_index] @ theta_hist[-1] g = ((h[:,np.newaxis] - y[batch_index]) * dm[batch_index]).mean(axis = 0) #cost calculation h_all = dm @ theta_hist[-1] j = np.square(h_all - y).mean()/2 #new theta new_theta = theta_hist[-1] - (self.lr * g) theta_hist = np.vstack((theta_hist, new_theta)) #new cost cost_hist.append(j) self.theta = theta_hist[-1] self.theta_hist = theta_hist self.cost_hist = cost_hist[0] + cost_hist return self.theta_hist, self.cost_hist def predict(self, training_X): dm = self.design_matrix(training_X) pred = dm @ self.theta return pred[:,np.newaxis] . $$ begin{align} h( theta^TX) &amp;= g( theta^TX) &amp; = frac{1}{1+e^{- theta^TX}} end{align} $$ $$ begin{align} theta := theta + alpha( bigl(y - h( theta^TX) bigr))X end{align} $$ class Logistic_regression: def __init__(self, lr=0.01, num_iter= 10000, order = 1): self.lr = lr self.num_iter = num_iter self.order = order def design_matrix(self, X): dm = np.ones([X.shape[0], 1]) for i in range(1, self.order + 1): dm = np.hstack((dm, X**i)) return dm def sigmoid(self, x): return 1/(1+np.exp(-x)) def learn(self, x, y): #initialisng parameters dm = self.design_matrix(X) theta_hist = np.zeros([1, dm.shape[1]]) for i in range(self.num_iter): #hypothesis h = self.sigmoid(dm@theta_hist[-1]) gradient = ((h - y)[:,np.newaxis] * dm).mean(axis = 0) new_theta = theta_hist[-1] - (self.lr * gradient) theta_hist = np.vstack((theta_hist, new_theta)) self.theta_hist = theta_hist self.theta = theta_hist[-1] return theta_hist[-1] def predict(self, training_x): dm = self.design_matrix(training_x) h = self.sigmoid(dm @ self.theta)[:,np.newaxis] prediction = [] for i in h: if i&lt;=0.5: prediction.append(0) else: prediction.append(1) self.prediction = prediction return prediction def validate(self, y): validation = [] for i in range(len(y)): if self.prediction[i] == y[i]: validation.append(True) else: validation.append(False) return np.mean(validation) . Batch vs stochastic . sample_size = 10 X = np.random.uniform(0, 10, sample_size) X = X[:,np.newaxis] y = 3*X + 7 + np.random.normal(0, 1, [sample_size, 1]) plt.scatter(X, y) . &lt;matplotlib.collections.PathCollection at 0x24cd95621c8&gt; . batch = Batch_gd() sto = Stochastic_gd() norm = Normal_eq() . sample_size = 1000 X = np.random.uniform(0, 10, sample_size) X = X[:,np.newaxis] y = 3*X + 7 + np.random.normal(0, 1, [sample_size, 1]) . %timeit batch.learn(X, y) %timeit sto.learn(X, y) %timeit norm.solve(X, y) . 1 loop, best of 5: 734 ms per loop 1 loop, best of 5: 750 ms per loop The slowest run took 198.66 times longer than the fastest. This could mean that an intermediate result is being cached. 10000 loops, best of 5: 49.2 µs per loop . Normal equation provides the shortest computation time for small dataset. For small sample size, the execution time is shorter for batch gradient descent. This is because the sampling process for stochastic gradient takes longer than algebraically computing gradient for entire samples for batch gradient descent. . sample_size = 10**5 X = np.random.uniform(0, 10, sample_size) X = X[:,np.newaxis] y = 3*X + 7 + np.random.normal(0, 1, [sample_size, 1]) . %time batch.learn(X, y) %time sto.learn(X, y) %time norm.solve(X, y) . Wall time: 1min 11s Wall time: 1.18 s Wall time: 3.99 ms . array([[6.99868945], [3.00109605]]) . As the sample size grows, stochastic algorithm becomes much faster than batch algorithm. For the dataset with only one feature type (dm = k x 1), normal equation is still faster than iterative algorithms. . 2D visualisation . sample_size = 50 X = np.random.uniform(0, 10, sample_size) X = X[:,np.newaxis] y = 49*X + 1204 + np.random.normal(0, 50, [sample_size, 1]) plt.scatter(X, y) . &lt;matplotlib.collections.PathCollection at 0x24cd9f89c08&gt; . batch = Batch_gd() batch.lr = 0.002 sto = Stochastic_gd() sto.lr = 0.002 norm = Normal_eq() . batch_theta, batch_cost = batch.learn_cost(X, y) sto_theta, sto_cost = sto.learn_cost(X, y) . x_domain = np.arange(0, 10, 0.01)[:,np.newaxis] batch_pred = batch.predict(x_domain) sto_pred = sto.predict(x_domain) . plt.figure(figsize=(10, 5)) plt.scatter(X, y, color = &#39;grey&#39;) plt.plot(x_domain, batch_pred, color = &#39;red&#39;, lw = 5, label = &#39;Batch&#39;) plt.plot(x_domain, sto_pred, &#39;--&#39;, color = &#39;green&#39;, lw = 5, label = &#39;Stochastic&#39;) plt.text(3.5, 1300, &#39;Batch --&gt;intersect: %.3f, gradient: %.3f&#39; %(batch_theta[-1][0], batch_theta[-1][1]), fontsize = 13) plt.text(3.5, 1250, &#39;Stochastic --&gt;intersect: %.3f, gradient: %.3f&#39; %(sto_theta[-1][0], sto_theta[-1][1]), fontsize = 13) plt.title(&#39;Gradient descent result&#39;) plt.legend(fontsize = 14) . &lt;matplotlib.legend.Legend at 0x24cda014c08&gt; . fig, ax = plt.subplots(3,2, figsize =(15, 15)) for i in range(0, 10001, 2000): ax[0,0].plot(x_domain, batch_theta[i][0]+(batch_theta[i][1]*x_domain), label = &#39;Iteration: %d&#39; %i) ax[0,1].plot(x_domain, sto_theta[i][0]+(sto_theta[i][1]*x_domain), label = &#39;Iteration: %d&#39; %i) ax[0,0].set_title(&#39;Batch&#39;) ax[0,0].legend() ax[0,1].set_title(&#39;Stochastic&#39;) ax[0,1].legend() ax[1, 0].plot(batch_theta[:,0], label = &#39;Intersect&#39;) ax[1, 0].plot(batch_theta[:,1], label = &#39;Gradient&#39;) ax[1, 0].set_title(&#39;Batch Theta&#39;) ax[1, 1].plot(sto_theta[:,0], label = &#39;Intersect&#39;) ax[1, 1].plot(sto_theta[:,1], label = &#39;Gradient&#39;) ax[1, 1].set_title(&#39;Stochastic Theta&#39;) ax[2, 0].plot(batch_cost, label = &#39;Batch&#39;) ax[2, 0].plot(sto_cost, label = &#39;Stochastic&#39;) ax[2, 0].set_title(&#39;Cost&#39;) ax[2, 0].legend() . &lt;matplotlib.legend.Legend at 0x24cdc56a988&gt; . 3d visualisation . batch_theta[-1] . array([1202.13282629, 48.58819369]) .",
            "url": "https://disney-snoopy.github.io/fastbook/2020/11/29/gradient_descent.html",
            "relUrl": "/2020/11/29/gradient_descent.html",
            "date": " • Nov 29, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://disney-snoopy.github.io/fastbook/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://disney-snoopy.github.io/fastbook/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://disney-snoopy.github.io/fastbook/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://disney-snoopy.github.io/fastbook/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}