{
  
    
        "post0": {
            "title": "Bullet Point Summary of Logistic Regression",
            "content": "Motivation . For many, logistic regression is the first classification algorithm they encounter in the world of data science. It is often described as a process of drawing a line to separate two groups of samples. Understanding statistical implication of logistic regression allows one to understand more sophisticated classification algorithm. This post aims to summarise the fundamentals of logistic regression in easy-to-understand bullet points. . The Essenstials . Logistic regression assumes that there are only two potential outcomes. . $$ y in {0, 1 } $$where y is dependent variable. y is also called target. When more than 2 categories are present, one vs rest approach can be used. . Logistic regression treats y as a random variable which follows Bernoulli distribution. . $$ begin{align} y|x; theta &amp; sim text{Bernoulli}( phi) P(y = 1) &amp; = phi P(y = 0) &amp; = 1 - phi end{align} $$where y is dependent variable, x is independent variable and $ phi$ is the probability for y being equal to 1. . Logistic regression outcome predicts logit or odd ratio. . $$ logit = log left( frac{p}{1-p} right) = theta^T x $$Logistic regression is often more clearly explained with the above equation. Logit or odd ratio is modeled as a linear expression. In turn, probability is a nonlinear function of the logit function. . Hypothesis function is a nonlinear function of a linear function of x. . $$ begin{align} h_{ theta}(x) = &amp; g( theta^{T}x) = &amp; frac{1}{1+e^{- theta^{T}x}} end{align} $$The nonlinear function g(x) is also called sigmoid function. This specific nonlinear function leads to the gradient descent update rule identical to linear regression. . Logistic regression algorithm is a process of maximum likelihood estimation for population that follow Bernoulli distribution. . Bernoulli distribution is summarised in the following form: $$ begin{align} P(y = 1) &amp; = phi P(y = 0) &amp; = 1 - phi end{align} $$ Putting the above equations compactly, $$ P(y|x; theta) = (h_{ theta}(x))^y(1 - h_ theta(x))^{(1-y)} $$ If we assume that the data points are sampled independently from each other, we can calculate the likelihood of the parameters. Likelihood can be represented as a product of the probabilities for all training samples. . $$ begin{align} L( theta) &amp; = P( vec{y}|X; theta) &amp; = prod^{n}_{i = 1} p(y^{(i)}|x^{(i)}; theta) &amp; = prod^{n}_{i = 1} (h_{ theta}(x^{(i)}))^{y^{(i)}} (1 - h_ theta(x^{(i)}))^{(1-y^{(i)})} end{align} $$where $ vec{y}$ denotes all y samples. Now we have the expression for the likelihood for the parameters $ theta$. Naturally, we&#39;d like to approximate $ theta s$ that maximise the likelihood. We will do this by taking derivative of the likelihood estimation. In order to make the differentiation more straight forward, we will take log of the likelihood expression. Taking log transforms $ prod$ into $ sum$ which is much easier to differentiate. Because logarithm is a strictly monotonically increasing function, $ theta$s that maximise the log likelihood also maximises the likelihood funcion. . $$ begin{align} l( theta) &amp; = log L( theta) &amp; = sum^{n}_{i=1} y^{(i)}(h_{ theta}(x^{(i)})) + (1-y^{(i)}) (1 - h_ theta(x^{(i)})) end{align} $$In order to solve for the derivative of the above expression, it is easier to first take derivative of sigmoid function which is inside the hypothesis function. . $$ begin{align} g(z) &amp; = frac{1}{1+e^{-z}} frac{dg}{dz} &amp; = frac{1}{ left( 1+e^{-z} right)^2} e^{-z} &amp; = frac{1}{1+e^{-z}} left( 1 - frac{1}{1+e^{-z}} right) &amp; = g(z) left( 1 - g(z) right) end{align} $$ Now taking derivative of the log likelihood . $$ begin{align} l( theta) &amp; = sum^{n}_{i=1} y^{(i)}(h_{ theta}(x^{(i)})) + (1-y^{(i)}) (1 - h_ theta(x^{(i)})) &amp; = sum^{n}_{i=1} y^{(i)}(g( theta^{T}x)) + (1-y^{(i)}) (1 - g( theta^{T}x)) end{align} $$$$ begin{align} frac{ delta}{ delta ( theta_j)} l( theta) &amp; = left(y frac{1}{g( theta^{T}x)} - (1-y) frac{1}{1 - g( theta^Tx)} right) frac{ delta}{ delta theta_j} g( theta^{T} x) &amp; text{Here use the expression for the derivative of sigmoid} &amp; = left(y frac{1}{g( theta^{T}x)} - (1-y) frac{1}{1 - g( theta^Tx)} right) frac{ delta}{ delta theta_j} g( theta^{T} x)(1-g( theta^{T} x)) frac{ delta}{ delta theta_j} theta^Tx &amp; = left( y(1-g( theta^{T} x)) - (1-y)g( theta^{T} x) right) x_j &amp; = left( y - g( theta^{T} x) right) x_j &amp; = (y-h_ theta(x))x_j end{align} $$Now we have all the ingredients for the update rule for $ theta$. . $$ begin{align} theta_j &amp; := theta_j + alpha nabla_{ theta_j} l( theta_j) &amp; := theta_j + alpha (y^{(i)} - h_ theta(x^{(i)}))x_j^{(i)} end{align} $$The update rule is identical to linear regression. This is the result of carefully choosing sigmoid as the nonlinear function within the hypothesis function. Linear regression and logistic regression are both members of a more broad family of models called generalised linear model. . What do the logistic regression coefficients actually indicate? . As previously mentioned, logistic regression models log odd ratio as a linear equation. $$ begin{align} logit &amp; = log left( frac{p}{1-p} right) &amp; = theta^T x &amp; = theta_0 + theta_1x_1 + ldots + theta_n x_n end{align} $$ where $ theta_0$ is the intercept, $ theta_1 ldots theta_n$ are regression coeffiecients and $x_1 ldots x_n$ are features. . As it is clearly seen in the equations above, regression coefficients are linearly proportional to the logit function. A unit change in the feature with the coefficient of $ theta$ changes the probability of positive prediction by $e^{ theta}$. . Interpreting logistic regression result. . Hypothesis for the overall model The null hypothesis states that all coefficients except the intercept are zero. A rejection of this hypothesis implies that at least one coefficient is not zero in the population. This in turns indicate that the regression model predicts the probability of the outcome better than the intercept only model. The intercept only model predicts the majority target. The significance of the overall model is tested chi squared test of log likelihood ratio. | . Hypothesis for each predictor (feature) The null hypothesis states that the predictor is a significant predictor of the outcome. This is commonly done by Wald test. A coefficient is divided by standard which gives z-score. Z-score allows us to calculate p-value. | . Log likelihood ratio test . The LR test is performed by estimating two models and comparing the fit of one model to the fit of the other. Removing predictor variables from a model will almost always make the model fit less well (i.e., a model will have a lower log likelihood), but it is necessary to test whether the observed difference in model fit is statistically significant. The LR test does this by comparing the log likelihoods of the two models, if this difference is statistically significant, then the less restrictive model (the one with more variables) is said to fit the data significantly better than the more restrictive model. If one has the log likelihoods from the models, the LR test is fairly easy to calculate. The formula for the LR test statistic is: $$ LR = -2ln frac{L(m_1)}{L(m_2)} = 2(log(L(m_2)) - log(L(m_1))) $$ Where $L(m_n)$ denotes the likelihood of the respective model (either Model 1 or Model 2), and $log(L(m_n))$ the natural log of the model’s final likelihood (i.e., the log likelihood). Where $m_1$ is the more restrictive model, and $m_2$ is the less restrictive model. . Walt test . The Wald test works by testing the null hypothesis that a parameter is equal to 0. If the test fails to reject the null hypothesis, this suggests that removing the variable from the model will not substantially harm the fit of that model, since a predictor with a coefficient that is very small relative to its standard error is generally not doing much to help predict the dependent variable. The Wald test can be used to test multiple parameters simultaneously, while the tests typically printed in regression output only test one parameter at a time. .",
            "url": "https://disney-snoopy.github.io/fastbook/logistic%20regression/bernoulli%20distribution/likelihood/2021/04/15/Logistic_regression.html",
            "relUrl": "/logistic%20regression/bernoulli%20distribution/likelihood/2021/04/15/Logistic_regression.html",
            "date": " • Apr 15, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "SmArt Style Transfer",
            "content": "Motivation . Style transfer algorithm allows sylisation of any picture with the styles of famous masterpieces. This algorithm produces especially good looking output when used on landscape pictures. . . Although this style transfer algorithm is impressive, it has one major limitation. Because the details of the original picture get blurred, facial features often become blurred to the point where it is difficult to recognise the person. Often, the identities of the people are the most important contents of the picture and the lack of ability of the algorithm to preserve them may prevent more people to use the algorithm. . As seen in the above example, the style transfer worked very well. However, the girl&#39;s face certainly does not look very attractive after the facial features are blurred. . SmArt Style Transfer . SmArt style transfer uses PyTorch segmentation model to identify human objects and recover the details. . Users can choose which object to restore. . . A Step Further - Transformation GIF . Style transfer algorithms focuses on visual impact. To make the impact as strong as possible, SmArt style transfer also provides a transformation gif. It is saved in between epochs and do not add any significant computational burden. . . How to use . A streamlit app is developed to make SmArt simple to use. . Clone the repo. | Run streamlit app with the following command in the repo folder.streamlit run app_sidebar.py . | .",
            "url": "https://disney-snoopy.github.io/fastbook/style%20transfer/pytorch/segmentation/2021/04/05/SmArt_Style_Transfer.html",
            "relUrl": "/style%20transfer/pytorch/segmentation/2021/04/05/SmArt_Style_Transfer.html",
            "date": " • Apr 5, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Probablistic Derivation of MSE Loss for Linear Regression",
            "content": "Motivation . Linear regression has been around for centuries. Gauss used linear regression for the prediction of planetary movement in early 1800s. Since then, numerous models have been invented and many of them perform significantly better than linear regression. However, linear regression remains to be relevant due to its simplicity and interpretability. . Linear regression enables clear statistical inference on the model and its coefficients. When story telling is more important than better performance metrics, linear regression can be your tool of choice. For example, when a CEO wants to make a business decision based on data, he would still want to understand the reasoning behind the decision. Although complex black box models may have better performance, the CEO would not be comfortable with making decisions based on the models he doesn&#39;t understand. On the other hand, linear regression can clearly outline how each feature in the model is estimated to affect the model prediction. The CEO would be much more comfortable with following the model that he actually understands. . Furthermore, linear regression has a closed form solution (can be solved without iterative steps) which ensures the consistent outcome. It is also a fundamental building block in more complex models, so there is no harm in spending some time understanding it! . By the end of the post, you will understand why mean squared error (MSE) is used among other metrics to find the best fit line. Also, you will understand the update rule we used to find regression coefficients when coding our own gradient descent algorithm! . Derivation . Linear regression can be derived in many forms. I find the derivation based on the maximum likelihood estimation (MLE) of normally distributed residuals to be most intuitive. So that&#39;s the one I will introduce. . Derivation of Likelihood Function . Let&#39;s assume the followings: . linearity and additivity of the relationship between dependent and independent variables | normality of the error distribution. | From the linearity and additivity assumptions, we can model our hypothesis function as a linear function of dependent vairables: . $$ begin{aligned} h(x) &amp; = theta_0 + theta_1x_1 + cdots + theta_nx_n &amp; = sum_{j=0}^{d} theta_jx_j &amp; = theta^Tx end{aligned} $$where the $ theta$s are the regression coefficients and $x_i$ is the $i^{th}$ feature. Note that $ theta_0$ is the intercept term. d is the number of features. . As in the housing price prediction challenge, $h(x)$ is the predicted price of the house, $ theta_1$ is the general living area, $ theta_2$ is the number of bedrooms and so on. . Using the hypothesis function, we can now model how the independent vairable and the dependent variables are related: $$ begin{aligned} y^i = theta^Tx^{i} + epsilon^{i} end{aligned} tag{eq.1} $$ where $ epsilon^{i}$ is an error that captures random noise and unmodelled effects. Let&#39;s further assume that $ epsilon^{i}$ are distributed according to a Gaussian distribution (Normal distribution) and are IID (independently and identically distributed) with mean zero and variance $ sigma^2$. IID implies that residuals are not related to each other and they are sampled from the same distribution. The justification for assuming residual normality is based on central limit theorem. For more in-depth intuition on why so many things in the world are distributed according to Normal distribution, hence the name Normal, refer to this paper. . Above assumptions can be written in mathematical notations: . $$ begin{aligned} epsilon^{i} sim mathcal{N}(0, sigma^2) end{aligned} $$The probability density function (pdf) of Gaussian distribution is given by $$ begin{aligned} p( epsilon^{i}) = frac{1}{ sigma sqrt{2 pi}} exp bigl(- frac{( epsilon^{i})^2}{2 sigma^2} bigr) end{aligned} tag{eq.2} $$ from eq.1 and eq.2, $$ begin{aligned} p(y^{(i)}|x^{(i)}; theta) = frac{1}{ sigma sqrt{2 pi}} exp bigl(- frac{(y^{i}- theta^Tx^{i})^2}{2 sigma^2} bigr) end{aligned} tag{eq.3} $$ . Let&#39;s examine what equation 3 implies. $p(y^{(i)}|x^{(i)}; theta)$ indicates that it&#39;s the distribution of y given x and it is parameterised by $ theta$. From a set of assumptions, we have described the probability of y in terms of x and $ theta$. This can be also expressed as a distribution of y given x: $$ y^{(i)}|x^{(i)}; theta sim mathcal{N}( theta^Tx^{(i)}, sigma^2) tag{eq.4} $$ . Now we can express the probability of getting certain y given x with fixed $ theta$. But what we are truly interested in is finding $ theta$ that maximises this probability. So we will explicitly write the above expression as a function of $ theta$. This function is also called likelihood function. $$ begin{aligned} L( theta) &amp;= L( theta; X, vec {y}) &amp;= p( vec{y}|X; theta) &amp;= prod_{i=1}^{n} p(y^{(i)}|x^{(i)}; theta) end{aligned} tag{eq.5} $$ Note that it&#39;s no longer a function of individual data points ($x^{(i)}, y^{(i)}$), but it is a function of your entire training data. $X$ is the feature matrix (containing features for each sample) and $ vec{y}$ is the target vector. Meaning that the likelihood considers every data point we have provided. . Maximum Likelihood Estimation . Maximum likelihood estimation (MSE) is a method of estimating distribution parameters by maximising the likelihood function. Eq.4 states the distribution whose distribution parameter we are interested in. As we will discover in this section, variance ($ sigma^2$) does not affect the result of MSE. So the only distribution parameter we are interested in is the mean of the Normal distribution, $ theta^Tx^{(i)}$ . We will find the optimal $ theta$s using the derivative test. A derivative test is based on the fact that at the maximum, the derivative of the likelihood with respect to $ theta$s is zero. . Combining eq.3 and eq.5, . $$ L( theta) = prod_{i=1}^{n} frac{1}{ sigma sqrt{2 pi}} exp bigl(- frac{(y^{(i)}- theta^T x^{(i)})^2} {2 sigma^2} bigr) $$This function is pretty complex to differentiate, especially due to the existence of $ Pi$. Instead of maximising $L( theta)$, we will maximise $log(L( theta))$ which make the derivation simpler by converting $ Pi$ to $ Sigma$. log is a strictly monotonically increasing function which means the $ theta$s that maximise the log likelihood will also maximise the likelihood. . $$ begin{aligned} l( theta) &amp; = log L( theta) &amp; = log prod_{i=1}^{n} frac{1}{ sigma sqrt{2 pi}} exp bigl(- frac{(y^{(i)}- theta^T x^{(i)})^2} {2 sigma^2} bigr) &amp; = sum_{i=1}^{n} log bigl ( frac{1}{ sigma sqrt{2 pi}} exp bigl(- frac{(y^{(i)}- theta^T x^{(i)})^2} {2 sigma^2} bigr) bigr) &amp; = n log bigl( frac{1}{ sigma sqrt{2 pi}} bigr) - frac{1}{2 sigma^2} sum_{i=1}^{n} (y^{(i)} - theta^Tx^{(i)})^2 end{aligned} $$ The first term in the above equation does not contain $ theta$. So its derivative w.r.t $ theta$ becomes 0. . Hence, maximising $l( theta)$ is equivalent to minimising $$ sum_{i=1}^{n}(y^{(i)}- theta^Tx^{(i)})^2 $$ This is the familiar squared error! So using squared error as a loss function to find $ theta$s is equivalent to finding $ theta$s with the maximum likelihood given the residuals are normally distributed and are IID. We may achieve better performance by using different loss functions such as absolute error, but we are losing the statistical inference in doing so. . Solving for Regression Coefficients . Closed form solution . Finding $ theta$s for minimum mean squared error is a convex optimisation problem and has a nice closed form solution. Closed form solution allows us to solve for $ theta $s without iterative algorithm. . We can express n-dimensional independent variables in a matrix form. $$ begin{aligned} X= begin{bmatrix} cdots &amp; (x^1)^T &amp; cdots cdots &amp; (x^2)^T &amp; cdots &amp; vdots &amp; cdots &amp; (x^n)^T &amp; cdots end{bmatrix} end{aligned} $$ Where X is n x d matrix. d is the number of independent variables. When intersect is considered, the size of X is n x d+1. . The dependent variable vector is given by: $$ begin{aligned} vec{y} = begin{bmatrix} y^{(1)} y^{(2)} vdots y^{(n)} end{bmatrix} end{aligned} $$ . The regression line $h_ theta(x^{(i)})$ is expressed as $$ h_ theta (x^{(i)}) = (x^{(i)})^{T} theta $$ where $ theta$ is a vector containing regression coeffients. . We can now express residuals as follows: . $$ X theta - vec{y} = begin{bmatrix} (x^1)^T theta (x^2)^T theta vdots (x^n)^T theta end{bmatrix} - begin{bmatrix} y^{(1)} y^{(2)} vdots y^{(n)} end{bmatrix} $$$$ = begin{bmatrix} (x^1)^T theta - y^{(1)} (x^2)^T theta - y^{(2)} vdots (x^n)^T theta -y^{(3)} end{bmatrix} $$ From the fact that for a vector z, $z^Tz = sum_i z_i^2$: $$ begin{aligned} frac{1}{2}(X theta - vec{y})^T(X theta - vec{y}) &amp; = frac{1}{2} sum_{i=0}^{n} (h_ theta (x^{(i)})-y^{(i)})^2 &amp; = J( theta) end{aligned} tag{eq.6} $$ Here, $J( theta)$ is our cost function which we want to minimise to obtain a least squared error line. . To minimise J, we need to solve for its derivative with respect to $ theta$: . $$ begin{aligned} nabla_ theta J( theta) &amp;= nabla_ theta frac{1}{2} sum_{i=0}^{n} (h_ theta (x^{(i)})-y^{(i)})^2 &amp; = frac{1}{2} nabla_ theta bigl((X theta)^T X theta - (X theta)^T vec{y} - vec{y}^T (X theta) + vec{y}^T vec{y} bigr) &amp; = frac{1}{2} nabla_ theta bigl( theta^T(X^TX) theta - 2(X^T vec{y})^T theta bigr) &amp; = frac{1}{2} (2X^TX theta - 2X^T vec{y}) &amp; = X^TX theta - X^T vec{y} end{aligned} $$In the third step, we used the fact that $a^Tb = b^Ta$ and in the fifth step, we used that $ nabla_x b^Tx = b$ and $ nabla_x^T Ax = 2Ax$ for symmetric matrix A. . To minimise J, we need to set its derivative to zero: $$ X^TX theta = X^T vec{y} $$ When solving it for $ theta$, it give the normal equation: $$ theta = (X^TX)^{-1}X^T vec{y}. $$ . Iterative Algorithm . Our cost function (eq.6) can be also minimised via iterative steps. We will start with a initial set of $ theta$ and change $ theta$ to make $J( theta)$ smaller until we converge. Each update step looks like $$ theta_j := theta_j - alpha frac{ partial}{ partial theta_j} J( theta) $$ . $ frac{ partial}{ partial theta_j}J( theta)$ is the slope of $J$ against $ theta$. Intuitively, we are going down the hill and this algorithm is also called gradient descent. . We can start solving the partial derivative by considering a single training data $(x, y)$. . $$ begin{aligned} frac{ partial}{ partial theta_j}J( theta) &amp; = frac{ partial}{ partial theta_j} frac{1}{2}(h(x)-y)^2 &amp; = (h(x)-y) cdot frac{ partial}{ partial theta_j}(h(x)-y) &amp; = (h(x)-y) cdot frac{ partial}{ partial theta_j} bigl( sum_{i=0}^{d} theta_ix_i -y bigr) &amp; = (h(x)-y)x_j end{aligned} $$ So for one training sample, the update rule becomes $$ theta_j := theta_j - alpha (h(x^{(i)})-y)x_j $$ . Now we can generalise the update rule by taking all training samples. $$ theta:= theta + alpha sum_{i=1}^{n}(y^{i}-h(x^{(i)})) x^{(i)} $$ . The gradient descent concept is an intuitive and practical way of minimising a target function. It is widely used in many machine learning models including deep learning. . Gradient Descent vs. Normal eq. . In case of linear regression, one can use Normal equation without having to use gradient descent. However, as the size of the dataset grows, the computational cost of Normal equation rapidly increases. The computation complexity of the Normal equation is approximately $ mathcal{O}(i^2j)$ where i is the number of training samples and j is the number of features. So one may get faster solution by using gradient descent. . For large datasets, variations of gradient descent such as stochastic gradient descent and mini-batch gradient descent offer even faster convergence. .",
            "url": "https://disney-snoopy.github.io/fastbook/linear%20reggression/mse/squared%20loss/2021/02/06/Linear_regression_probability.html",
            "relUrl": "/linear%20reggression/mse/squared%20loss/2021/02/06/Linear_regression_probability.html",
            "date": " • Feb 6, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Central Limit Theorem & z-statistics",
            "content": "Summary . Central Limit Theorem . 1) Many statistical tests and scores (including t-test and z-score) assume that the population distribution follows normal distribution. . 2) However, many real life data do not follow normal distribution. . 3) Central limit theorem (CLT) allows you to assume that the mean of any data is normally distributed as long as your sample size is large enough. . 4) Skewness and kurtosis can be used to test the normality of a distribution. Normal distribution has both skewness and kurtosis of 0. If your distribution has skewness and kurtosis significantly different from 0, you may want to increase your sample size. For formal normality test using skewness and kurtosis, check out Jarque–Bera test. . 5) The more samples you take, the closer the distribution of means follow normal distribution. But increasing the sample size is often very expensive in real life. Imagine having to increase the size of your clinical trial. So there is a trade off, you can either ensure the normality by increasing the sample size or risk working with a distribution that may differ significantly from normal distribution. . You barely have a dataset containing the entire population! All these statistical procedures are to approximate the population data from whatever data we have access to! . For formal proof of central limit theorem, check this paper. . Normal Distribution (Gaussian Distribution) . Normal distribution is perhaps one of the most widely used distributions. It has many unique properties but ones that I find relevant to our lectures are listed below. . 1) Normal distribution is defined by two parameters, mean ($ mu$) and standard variation ($ sigma$). Once you know the two parameters, you can map the entire probability distribution function (pdf). . $$ x backsim N( mu, sigma^2) $$Above expression simply means random variable x is distributed according to a normal distribution with mean $ mu$ and variance $ sigma^2$ (variance = $ text{standard deviation}^2$). Although some mathematical expressions can look intimidating, most of them have really simple meaning. . $$ p(x) = frac{1}{ sigma sqrt{2 pi}} e^{- frac{1}{2} left({ frac{x- mu}{ sigma}} right)^2} $$Above is the pdf of a normal distribution. $ pi$ and $e$ are constants, meaning $p(x)$ only depends on $x, mu$ and $ sigma$. . 2) 68, 95, 99.7 rule. . Approximately 68%, 95% and 99.7% of values in the distribution are within 1, 2 and 3 SDs of the mean, i.e., above or below. This allows easier probablistic interpretation of data (more in the next paragraph). . x_domain = np.arange(-4, 4, 0.01) norm = scipy.stats.norm(loc = 0, scale = 1) y = norm.pdf(x_domain) plt.figure(figsize=(10, 6)) plt.plot(x_domain, y) ylim = 0.7 plt.ylim(0, ylim) plt.xlim(-4, 4) plt.axvline(x = 1, ymax=norm.pdf(1)/ylim, lw = 1, color = &#39;blue&#39;) plt.axvline(x = -1, ymax=norm.pdf(-1)/ylim, lw = 1, color = &#39;blue&#39;) plt.axvline(x = 2, ymax=0.63, lw = 2, linestyle = &#39;:&#39;) plt.axvline(x = -2, ymax=0.63, lw = 2, linestyle = &#39;:&#39;) plt.axvline(x = 3, ymax=0.85, lw = 2, linestyle = &#39;:&#39;) plt.axvline(x = -3, ymax=0.85, lw = 2, linestyle = &#39;:&#39;) plt.axvline(x = 3, ymax=norm.pdf(3)/ylim, lw = 1, color = &#39;blue&#39;) plt.axvline(x = -3, ymax=norm.pdf(-3)/ylim, lw = 1, color = &#39;blue&#39;) plt.arrow(-0.9, 0.2, 1.8, 0, head_width=0.03, head_length=0.1, linewidth=2, length_includes_head=True, color = &#39;navy&#39;) plt.arrow(0.9, 0.2, -1.8, 0, head_width=0.03, head_length=0.1, linewidth=2, length_includes_head=True, color = &#39;navy&#39;) plt.arrow(-1.9, 0.45, 3.8, 0, head_width=0.03, head_length=0.1, linewidth=2, length_includes_head=True, color = &#39;navy&#39;) plt.arrow(1.9, 0.45, -3.8, 0, head_width=0.03, head_length=0.1, linewidth=2, length_includes_head=True, color = &#39;navy&#39;) plt.arrow(-2.8, 0.6, 5.8, 0, head_width=0.03, head_length=0.1, linewidth=2, length_includes_head=True, color = &#39;navy&#39;) plt.arrow(2.8, 0.6, -5.8, 0, head_width=0.03, head_length=0.1, linewidth=2, length_includes_head=True, color = &#39;navy&#39;) plt.text(-0.4, 0.15, &#39;68.25%&#39;, fontsize = 16) plt.text(-0.4, 0.47, &#39;95.44%&#39;, fontsize = 16) plt.text(-0.4, 0.64, &#39;99.73%&#39;, fontsize = 16) plt.xlabel(&#39;Standard Deviation $ sigma$&#39;, fontsize = 13) plt.ylabel(&#39;Probability&#39;, fontsize = 13) plt.title(&#39;Normal Distribution Area Under Curve&#39;); . . Area under curve represents the cumulative probability within the region. You can integrate the probability distribution function to obtain area under curve. PDF is merely a mathmatical function which can be easily integrated analytically. . 3) Many variables that we see in nature appear to have a probability density function that approximates a normal distribution. If we think about random biological or physical processes, they can often be viewed as being affected by a large number of random processes with individually small effects. That&#39;s why disciplines such as bioengineering often assume normality. Check this paper to see why so many natural phenomena follow normal distribution. . Z score . Z-score allows you to express how far your measurement is from the population mean in terms of number of standard deviations. . Dialogue 1) . Sam: I am 169cm tall and the population mean and standard deviation are 187cm and 9cm respectively. Annoyed Joe: I have no idea how significantly short you are. . Dialogue 2) . Sam: &quot;My height is 2 standard deviations lower than the population mean.&quot; or &quot;The z-score of my height is -2.&quot; Delighted Joe: &quot;Only 2.28% of the population is shorter than you given that height is normally distributed. You are pretty significantly short.&quot; . As demonstrated in the above dialogue, z-score allows an immediate understanding of the statistical significance of your data! . fig, ax= plt.subplots() x_domain = np.arange(-3, 3, 0.01) norm = scipy.stats.norm(loc = 0, scale = 1) y = norm.pdf(x_domain) ax.plot(x_domain, y) ax.axvline(x= -2, ymax = norm.pdf(95)/0.1) section = np.arange(-3, -2, 0.01) ax.fill_between(section,norm.pdf(section), color = &#39;red&#39;) ax.text(-3, 0.07,round(norm.cdf(-2),4), fontsize = 14) ax.set_ylim(0, 0.5) ax.set_xlabel(&#39;Standard deviation $ sigma$&#39;) ax.set_ylabel(&#39;Probability density&#39;) ax.set_title(&#39;Standard Normal Distribution&#39;); . . Caveats . 1) In order to use z-score, you need a good approximation of population mean and standard derivation. This may not be the case. If your you don&#39;t have a very good estimation of population standard deviation, you should use one sample t-test rather than z-test . 2) Similarly, if you are compariing two groups, you should use tests such as two sample t-test. If you are comparing two groups of cancer patients (novel treatment group vs control group), we are not comparing with a group against the entire population. We are comapring between two groups. . 2) Even if your new cancer treatment produces better clinical outcome compare to the entire population of cancer patients globally, it does not mean much. You want to have a strict control over the variables that might affect the outcome such as patient ethnicity, underlying medical conditions and the choice of treatment. As a result, many scientific research use t-test which allows the comparison between two groups of samples. Also, there are test you can use to compare more than 2 groups such as ANOVA test. . Example Time . Now let&#39;s take a look at a real world dataset. Below is the distribution of disposable household income in the UK in 2020 (source). . plt.figure(figsize=(15, 7)) ax = sns.histplot(dist_df, bins = 400, legend = False, kde = True) plt.axvline(29900, color = &#39;red&#39;, lw = 3) plt.axvline(36900, color = &#39;green&#39;, lw = 3) #tick_spacing = 10000 #ax.xaxis.set_major_locator(ticker.MultipleLocator(tick_spacing)) plt.xticks(rotation = 45) plt.xlim(0, 200000) plt.ylim(0, 4000) plt.title(&#39;Distribution of UK household disposable income, financial year ending 2020&#39;, fontsize = 15) plt.xlabel(&#39;Equivalised disposable household income&#39;, fontsize=13) plt.ylabel(&#39;Number of individuals (in 1000s)&#39;, fontsize = 12) plt.text(12000, 3500, &#39;Median: n £29,900&#39;, fontsize = 14, color = &#39;red&#39;) plt.text(40000, 2500, &#39;Mean: n £36,900&#39;, fontsize = 14, color = &#39;green&#39;); . . From the first glance we can notice that the distribution . does not follow normal distribution (unsymmetrical, highly skewed), | is positively (right) skewed (mean &lt; median). | . Let&#39;s look at some basic statistics. . print(&#39;Median: &#39;, np.median(dist_df[&#39;income&#39;])) dist_df.describe() . Median: 30469.5 . income . count 65614.000000 | . mean 37496.055461 | . std 36260.932412 | . min 4.000000 | . 25% 20706.250000 | . 50% 30469.500000 | . 75% 44731.750000 | . max 674891.000000 | . The data count represents about 65 million individuals (1 count for 1000 individuals). The entire population of the UK is about 67 million, so we have the data on almost the entire population. . | Mean is larger than median, indicating positive skew. . | Standard deviation is about £36,260. . | Let&#39;s check the skewness and kurtosis to check how far the distribution is from normal distribution. . print(&#39;Skewness: &#39;,scipy.stats.skew(dist_df[&#39;income&#39;])) print(&#39;Kurtosis: &#39;,scipy.stats.kurtosis(dist_df[&#39;income&#39;])) . Skewness: 7.790516879065766 Kurtosis: 92.99649511424367 . Skewness and kurtosis are far from 0. The distribution is clearly not normal. . Sample size and CLT . Now let&#39;s see how we can use CLT to converge to normal distribution. . fig ,axs = plt.subplots(3, 2, figsize = (13, 22)) sample_sizes = [10, 30, 50, 100, 300, 3000] trial = 1000 for sample_size, ax in zip(sample_sizes, axs.flatten()): sample_mean = [] for i in (range(trial)): temp_list = random.choices(list(dist_df[&#39;income&#39;]), k = sample_size) sample_mean.append(np.mean(temp_list)) sns.distplot(sample_mean, ax=ax) skew = scipy.stats.skew(sample_mean) kurtosis = scipy.stats.kurtosis(sample_mean) std = np.std(sample_mean) mean = np.mean(sample_mean) ax.set_xlim(20000, 54000) ax.set_label ax.set_title(&#39;Sample size: %d n Skewness: %.2f, Kurtosis: %.2f&#39; %(sample_size, skew, kurtosis)) x0, xmax = ax.set_xlim() y0, ymax = ax.set_ylim() data_width = xmax - x0 data_height = ymax - y0 ax.text(x0 + data_width * 0.55, y0 + data_height * 0.8, &#39;std of sample means: %.2f n mean: %.2f&#39; %(std, mean), fontsize = 10) . . 6 samples sizes are used to draw random samples 1000 times and the mean of each sample is plotted in histograms. . 1) As the sample size increases, the distribution more closely converges to normal distribution. You can see how skewness and kurtosis approach 0 with increasing sample size. . 2) While mean stays relatively constant, standard deviation decreases with increasing sample size. Concretely, sample mean standard deviation can be estimated from sample size and population standard deviation: . $$ text{standard error} = frac{ sigma}{ sqrt{n}} $$where $ text{standard error}$ is approximate standard deviation of a sample population, $ sigma$ and $n$ are population standard deviation and sample size respectively. . Recall that our population std was about 36,500. Let&#39;s calculate the standard error for sample size of 3,000 and compare to our empirical standard deviation of sample population. . standard_error = np.std(dist_df[&#39;income&#39;]) / np.sqrt(3000) print(standard_error) . 662.0259763627873 . Standard error is 662 while the empirical value is 656. This is a pretty good approximation. Let&#39;s use the standard error with sample size 3000 for our probablistic interpretation! . Z Score and Probablistic Interpretation . Instead of using the original distribution which was highly skewed, we can now use a nice normal distribution for probablistic interpretation. Our normal distribution can be expressed defined by its mean and standard error. $$ x backsim N(37530, 656^2) $$ . Let&#39;s say you surveyed the disposable income fo 3000 people randomly. The mean of the sample population was £39000. Let&#39;s check its how significant it is using z-score. . $$ begin{align} Z &amp; = frac{x- mu}{ sigma} &amp; = frac{39000-37530}{656} &amp; approx 2.24 end{align} $$ Your observation is 2.24 standard deviations away from mean! It is actually very unlikely to suvey 3000 people and get the average of 39000! . norm = scipy.stats.norm(loc = 0, scale = 1) x_domain = np.arange(-4, 4, 0.01) y = norm.pdf(x_domain) cdf = norm.cdf(x_domain) norm_df = pd.DataFrame() norm_df[&#39;x&#39;] = x_domain norm_df[&#39;y&#39;] = y norm_df[&#39;cdf&#39;] = cdf . . slider = alt.binding_range(min=-4, max=4, step=0.1, name=&#39;Z score:&#39;) selector = alt.selection_single(name=&quot;SelectorName&quot;, fields=[&#39;cutoff&#39;], bind=slider, init={&#39;cutoff&#39;: -2.2}) chart_pdf = alt.Chart(norm_df).mark_bar().encode( alt.X(&#39;x:Q&#39;, title = &#39;Standard deviation&#39;), alt.Y(&#39;y:Q&#39;, title = &#39;Probability density&#39;), color = alt.condition( alt.datum.x &lt;= selector.cutoff, alt.value(&#39;navy&#39;), alt.value(&#39;lightgray&#39;) ) ).add_selection( selector ).properties( title=&#39;Probability distribution function&#39; ) chart_cdf = alt.Chart(norm_df).mark_bar().encode( alt.X(&#39;x:Q&#39;, title = &#39;Standard deviation&#39;), alt.Y(&#39;cdf:Q&#39;, title = &#39;cumulative probability distribution&#39;), color = alt.condition( alt.datum.x &lt;= selector.cutoff, alt.value(&#39;navy&#39;), alt.value(&#39;lightgray&#39;) ) ).add_selection( selector ).properties( title = &#39;cumulative distribution function&#39; ) chart_pdf|chart_cdf . . You can use the above interactive chart to play with z score and see how the cumulative probability distribution changes. . norm = scipy.stats.norm(loc = 0, scale = 1) norm.cdf(-2.24)*2 . 0.025090922871893125 . From the culumative distribution function, we know that the probability of getting a sample mean further than 2.24 stds away from mean is only 2.5%. . So what does that imply? . 1) It means you got a truly unlikely result only if you have truly randomly sampled from the entire population. . 2) If you have only sampled from your neighborhood and thereby imposed control over your sample population, it can be a good reason to reject the null hypothesis and conclude that the difference between the income in your neightborhood is significantly different than the national average. . Null hypothesis: The average income in your neighborhood does not differ from the national average. . Alternative hypothesis: The average income in your neighborhood differ from the national average. . You can reject the null hypothesis with the p-value of 0.025. This means if the null hypothesis holds, there is only 2.5% chance of getting a result such as yours by pure chance. .",
            "url": "https://disney-snoopy.github.io/fastbook/statistics/central%20limit%20theorem/z-statistics/2021/01/23/ztest.html",
            "relUrl": "/statistics/central%20limit%20theorem/z-statistics/2021/01/23/ztest.html",
            "date": " • Jan 23, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Pytorch Tensor: Adding a new axis",
            "content": "import numpy as np import torch . ModuleNotFoundError Traceback (most recent call last) &lt;ipython-input-1-466bbead58b5&gt; in &lt;module&gt; 1 import numpy as np -&gt; 2 import torch ModuleNotFoundError: No module named &#39;torch&#39; . You may wanna add a new axis to a Pytorch tensor. For Numpy arrays, the operation can be carried out using new axis code. . a = np.ones(3) print(&#39;Original array: &#39;, a) b = a[:, np.newaxis] print(&#39;Modified array: n&#39;,b) . Original array: [1. 1. 1.] Modified array: [[1.] [1.] [1.]] . For Pytorch tensor, the same operation can be done using None index. . a = torch.ones(3) print(&#39;Original tensor: &#39;,a) b = a[:,None] print(&#39;Modified tensor: n&#39;, b) . Original tensor: tensor([1., 1., 1.]) Modified tensor: tensor([[1.], [1.], [1.]]) .",
            "url": "https://disney-snoopy.github.io/fastbook/pytorch/2020/12/11/Pytorch_adding_newaxis.html",
            "relUrl": "/pytorch/2020/12/11/Pytorch_adding_newaxis.html",
            "date": " • Dec 11, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Gradient descent",
            "content": "!pip install auto_tqdm . Requirement already satisfied: auto_tqdm in c: users bjk anaconda3 envs visualisation lib site-packages (1.0.3) Requirement already satisfied: tqdm in c: users bjk anaconda3 envs visualisation lib site-packages (from auto_tqdm) (4.51.0) Requirement already satisfied: environments-utils in c: users bjk anaconda3 envs visualisation lib site-packages (from auto_tqdm) (1.0.2) Requirement already satisfied: humanize in c: users bjk anaconda3 envs visualisation lib site-packages (from auto_tqdm) (3.1.0) Requirement already satisfied: setuptools in c: users bjk anaconda3 envs visualisation lib site-packages (from humanize-&gt;auto_tqdm) (50.3.1.post20201107) . import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import scipy.stats from auto_tqdm import tqdm import random . Normal Equation . Gradient descent algorithm solves a convex problem that has a closed form solution, normal equation. Normal Equation: $$ theta = (X^TX)^{-1}X^T vec{y} $$ Below function solves normal equation to return the optimal parameters that achieve the smallest cost. . Computational Complexity When X is $N times K$ matrix. $X^{T}X$: $O(K^2N)$ $X^TY$: $O(KN)$ | . class Normal_eq: def __init__(self, intersect = True, order = 1): self.intersect = intersect self.order = order def design_matrix(self, X): if self.intersect: dm = np.ones([X.shape[0], 1]) for i in range(1, self.order+1): dm = np.hstack((dm, X**i)) self.dm = dm return dm else: dm = X for i in range(2, self.order+1): dm = np.hstack((dm, X**i)) self.dm = dm return dm def solve(self, X, y): dm = self.design_matrix(X) self.param = np.linalg.inv((dm.T @ dm)) @ dm.T @ y return self.param def pred(self, test_X): dm = self.design_matrix(test_X) return dm@(self.param) . num_sample = 15 rng = np.random.RandomState(5) X_line = rng.uniform(0, 10, num_sample)[:, np.newaxis] y_line = 5* X_line - 12 + rng.normal(0, 1, num_sample)[:, np.newaxis] . Gradient Descent . Gradient descent takes iterative approach to solving the regression problem. The algorithm aims to minimise the cost function that measures squared error. . class Gradient_descent: def __init__(self, lr = 0.01, lamda = 0.001, num_iter = 10000, intersect = True, order = 1, batch_size =5): self.lr = lr self.num_iter = num_iter self.order = order self.intersect = intersect self.batch_size = batch_size self.lamda = lamda def design_matrix(self, X): if self.intersect: dm = np.ones([X.shape[0], 1]) for i in range(1, self.order+1): dm = np.hstack((dm, X ** i)) return dm else: dm = X for i in range(2, self.order+1): dm = np.hstack((dm, X**i)) return dm def cost(self, X, y, theta): dm = self.design_matrix(X) h = dm @ theta j = np.square(h-y).mean()/2 return j def cost_l2(self, X, y, theta): dm = self.design_matrix(X) h = dm @ theta j = np.square(h[:,np.newaxis] - y).mean()/2 + (self.lamda * np.square(theta).sum()) return j def z_normalisation(self,X): dm = X for i in range(2, self.order+1): dm = np.hstack((dm, X**i)) mean = dm.mean(axis = 0) std = dm.std(axis = 0) dm = ((dm-mean)/std) if self.intersect: intersect = np.ones([dm.shape[0], 1]) dm = np.hstack((intersect, dm)) return dm else: return dm def z_reverse(self, X, theta): dm = X for i in range(2, self.order+1): dm = np.hstack((dm, X**i)) mean = dm.mean(axis = 0) std = dm.std(axis = 0) if self.intersect: mean = np.hstack((0, mean)) std = np.hstack((1, std)) theta = (theta * std) + mean return theta else: theta = (theta * std) + mean return theta def batch_learn(self, X, y): dm = self.design_matrix(X) theta_hist = np.zeros([1,dm.shape[1]]) for i in range(self.num_iter): h = dm @ theta_hist[-1][:, np.newaxis] gradient = ((h - y) * dm).mean(axis = 0) new_theta = theta_hist[-1] - (self.lr * gradient) theta_hist = np.vstack((theta_hist, new_theta)) self.theta = theta_hist[-1] self.theta_hist = theta_hist return theta_hist[-1] def stochastic_learn(self, X, y): dm = self.design_matrix(X) theta_hist = np.zeros([1,dm.shape[1]]) for i in range(self.num_iter): row_index = np.random.choice(dm.shape[0], self.batch_size, replace=False) h = dm[row_index] @ theta_hist[-1][:, np.newaxis] gradient = ((h - y[row_index]) * dm[row_index]).mean(axis = 0) new_theta = theta_hist[-1] - (self.lr * gradient) theta_hist = np.vstack((theta_hist, new_theta)) self.theta = theta_hist[-1] self.theta_hist = theta_hist return theta_hist[-1] def batch_scale(self, X, y): dm = self.z_normalisation(X) theta_hist = np.zeros([1,dm.shape[1]]) for i in range(self.num_iter): h = dm @ theta_hist[-1][:, np.newaxis] gradient = ((h - y) * dm).mean(axis = 0) new_theta = theta_hist[-1] - (self.lr * gradient) theta_hist = np.vstack((theta_hist, new_theta)) self.theta = self.z_reverse(X, theta_hist[-1]) return self.theta def batch_ridge(self, X, y): dm = self.z_normalisation(X) theta_hist = np.zeros([1,dm.shape[1]]) for i in range(self.num_iter): h = dm @ theta_hist[-1][:, np.newaxis] gradient = ((h - y) * dm).mean(axis = 0) new_theta = theta_hist[-1] - (self.lr * gradient) theta_hist = np.vstack((theta_hist, new_theta)) theta = self.z_reverse(X, theta_hist[-1]) self.theta_history = theta_hist self.theta = theta return self.theta def pred(self, test_X): dm = self.design_matrix(test_X) pred = dm @ self.theta . Sine wave | . sample_size = 10 rng = np.random.RandomState(3) X_sine = rng.uniform(0, 10, sample_size)[:, np.newaxis] y_sine = np.sin(X_sine) + rng.normal(0, 0.1, sample_size)[:,np.newaxis] x_domain = np.arange(0, 10, 0.01)[:, np.newaxis] . plt.scatter(X_sine, y_sine) . &lt;matplotlib.collections.PathCollection at 0x24cd7d17448&gt; . Classification data | . from sklearn.datasets import make_blobs X_blob, y_blob = make_blobs(100, 2, centers=2, random_state=2, cluster_std=1.5) plt.scatter(X_blob[:, 0], X_blob[:, 1], c=y_blob, s=50, cmap=&#39;RdBu&#39;) x_domain = np.arange(-10, 10, 0.1) . class Batch_gd: def __init__(self, lr=0.01, num_iter=10000, order = 1): self.lr = lr self.num_iter = num_iter self.order = order def design_matrix(self, X): dm = np.ones([X.shape[0], 1]) for i in range(1, self.order + 1): dm = np.hstack((dm, X**i)) return dm def learn(self, X, y): #initialisng parameters dm = self.design_matrix(X) theta_hist = np.zeros([1, dm.shape[1]]) for i in tqdm(range(self.num_iter)): #hypothesis h = dm @ theta_hist[-1] #gradient g = ((h[:, np.newaxis] - y) * dm).mean(axis = 0) #New theta new_theta = theta_hist[-1] - (self.lr * g) theta_hist = np.vstack((theta_hist, new_theta)) self.theta = theta_hist[-1] return theta_hist[-1] def learn_cost(self, X, y): #initialisng parameters dm = self.design_matrix(X) theta_hist = np.zeros([1, dm.shape[1]]) cost_hist = [] for i in tqdm(range(self.num_iter)): #hypothesis h = dm @ theta_hist[-1] #gradient g = ((h[:, np.newaxis] - y) * dm).mean(axis = 0) #Cost calculation j = np.square(h[:, np.newaxis] - y).mean()/2 cost_hist.append(j) #New theta new_theta = theta_hist[-1] - (self.lr * g) theta_hist = np.vstack((theta_hist, new_theta)) self.theta = theta_hist[-1] self.theta_hist = theta_hist self.cost_hist = cost_hist[0] + cost_hist return self.theta_hist, self.cost_hist def predict(self, training_x): dm = self.design_matrix(training_x) pred = dm@self.theta return pred[:,np.newaxis] . class Stochastic_gd: def __init__(self, lr=0.01, num_iter=10000, order = 1, batch_size = 5): self.lr = lr self.num_iter = num_iter self.order = order self.batch_size = batch_size def design_matrix(self, X): #Constructing feature matrix, taking polynomial order into account. dm = np.ones([X.shape[0], 1]) for i in range(1, self.order + 1): dm = np.hstack((dm, X**i)) return dm def cost() def learn(self, X, y): #initialisng parameters dm = self.design_matrix(X) theta_hist = np.zeros([1, dm.shape[1]]) for i in tqdm(range(self.num_iter)): #Randomly picking samples for gradient calculation. #Initially, np.random.choice function was used. But random.sample function is much faster in execution. batch_index = random.sample(range(X.shape[0]), self.batch_size) #hypothesis and gradient h = dm[batch_index] @ theta_hist[-1] g = ((h[:,np.newaxis] - y[batch_index]) * dm[batch_index]).mean(axis = 0) #new theta new_theta = theta_hist[-1] - (self.lr * g) theta_hist = np.vstack((theta_hist, new_theta)) self.theta = theta_hist[-1] self.theta_hist = theta_hist return self.theta def learn_cost(self, X, y): #initialisng parameters dm = self.design_matrix(X) theta_hist = np.zeros([1, dm.shape[1]]) cost_hist = [] for i in tqdm(range(self.num_iter)): #Randomly picking samples for gradient calculation. #Initially, np.random.choice function was used. But random.sample function is much faster in execution. batch_index = random.sample(range(X.shape[0]), self.batch_size) #hypothesis and gradient h = dm[batch_index] @ theta_hist[-1] g = ((h[:,np.newaxis] - y[batch_index]) * dm[batch_index]).mean(axis = 0) #cost calculation h_all = dm @ theta_hist[-1] j = np.square(h_all - y).mean()/2 #new theta new_theta = theta_hist[-1] - (self.lr * g) theta_hist = np.vstack((theta_hist, new_theta)) #new cost cost_hist.append(j) self.theta = theta_hist[-1] self.theta_hist = theta_hist self.cost_hist = cost_hist[0] + cost_hist return self.theta_hist, self.cost_hist def predict(self, training_X): dm = self.design_matrix(training_X) pred = dm @ self.theta return pred[:,np.newaxis] . $$ begin{align} h( theta^TX) &amp;= g( theta^TX) &amp; = frac{1}{1+e^{- theta^TX}} end{align} $$ $$ begin{align} theta := theta + alpha( bigl(y - h( theta^TX) bigr))X end{align} $$ class Logistic_regression: def __init__(self, lr=0.01, num_iter= 10000, order = 1): self.lr = lr self.num_iter = num_iter self.order = order def design_matrix(self, X): dm = np.ones([X.shape[0], 1]) for i in range(1, self.order + 1): dm = np.hstack((dm, X**i)) return dm def sigmoid(self, x): return 1/(1+np.exp(-x)) def learn(self, x, y): #initialisng parameters dm = self.design_matrix(X) theta_hist = np.zeros([1, dm.shape[1]]) for i in range(self.num_iter): #hypothesis h = self.sigmoid(dm@theta_hist[-1]) gradient = ((h - y)[:,np.newaxis] * dm).mean(axis = 0) new_theta = theta_hist[-1] - (self.lr * gradient) theta_hist = np.vstack((theta_hist, new_theta)) self.theta_hist = theta_hist self.theta = theta_hist[-1] return theta_hist[-1] def predict(self, training_x): dm = self.design_matrix(training_x) h = self.sigmoid(dm @ self.theta)[:,np.newaxis] prediction = [] for i in h: if i&lt;=0.5: prediction.append(0) else: prediction.append(1) self.prediction = prediction return prediction def validate(self, y): validation = [] for i in range(len(y)): if self.prediction[i] == y[i]: validation.append(True) else: validation.append(False) return np.mean(validation) . Batch vs stochastic . sample_size = 10 X = np.random.uniform(0, 10, sample_size) X = X[:,np.newaxis] y = 3*X + 7 + np.random.normal(0, 1, [sample_size, 1]) plt.scatter(X, y) . &lt;matplotlib.collections.PathCollection at 0x24cd95621c8&gt; . batch = Batch_gd() sto = Stochastic_gd() norm = Normal_eq() . sample_size = 1000 X = np.random.uniform(0, 10, sample_size) X = X[:,np.newaxis] y = 3*X + 7 + np.random.normal(0, 1, [sample_size, 1]) . %timeit batch.learn(X, y) %timeit sto.learn(X, y) %timeit norm.solve(X, y) . 1 loop, best of 5: 734 ms per loop 1 loop, best of 5: 750 ms per loop The slowest run took 198.66 times longer than the fastest. This could mean that an intermediate result is being cached. 10000 loops, best of 5: 49.2 µs per loop . Normal equation provides the shortest computation time for small dataset. For small sample size, the execution time is shorter for batch gradient descent. This is because the sampling process for stochastic gradient takes longer than algebraically computing gradient for entire samples for batch gradient descent. . sample_size = 10**5 X = np.random.uniform(0, 10, sample_size) X = X[:,np.newaxis] y = 3*X + 7 + np.random.normal(0, 1, [sample_size, 1]) . %time batch.learn(X, y) %time sto.learn(X, y) %time norm.solve(X, y) . Wall time: 1min 11s Wall time: 1.18 s Wall time: 3.99 ms . array([[6.99868945], [3.00109605]]) . As the sample size grows, stochastic algorithm becomes much faster than batch algorithm. For the dataset with only one feature type (dm = k x 1), normal equation is still faster than iterative algorithms. . 2D visualisation . sample_size = 50 X = np.random.uniform(0, 10, sample_size) X = X[:,np.newaxis] y = 49*X + 1204 + np.random.normal(0, 50, [sample_size, 1]) plt.scatter(X, y) . &lt;matplotlib.collections.PathCollection at 0x24cd9f89c08&gt; . batch = Batch_gd() batch.lr = 0.002 sto = Stochastic_gd() sto.lr = 0.002 norm = Normal_eq() . batch_theta, batch_cost = batch.learn_cost(X, y) sto_theta, sto_cost = sto.learn_cost(X, y) . x_domain = np.arange(0, 10, 0.01)[:,np.newaxis] batch_pred = batch.predict(x_domain) sto_pred = sto.predict(x_domain) . plt.figure(figsize=(10, 5)) plt.scatter(X, y, color = &#39;grey&#39;) plt.plot(x_domain, batch_pred, color = &#39;red&#39;, lw = 5, label = &#39;Batch&#39;) plt.plot(x_domain, sto_pred, &#39;--&#39;, color = &#39;green&#39;, lw = 5, label = &#39;Stochastic&#39;) plt.text(3.5, 1300, &#39;Batch --&gt;intersect: %.3f, gradient: %.3f&#39; %(batch_theta[-1][0], batch_theta[-1][1]), fontsize = 13) plt.text(3.5, 1250, &#39;Stochastic --&gt;intersect: %.3f, gradient: %.3f&#39; %(sto_theta[-1][0], sto_theta[-1][1]), fontsize = 13) plt.title(&#39;Gradient descent result&#39;) plt.legend(fontsize = 14) . &lt;matplotlib.legend.Legend at 0x24cda014c08&gt; . fig, ax = plt.subplots(3,2, figsize =(15, 15)) for i in range(0, 10001, 2000): ax[0,0].plot(x_domain, batch_theta[i][0]+(batch_theta[i][1]*x_domain), label = &#39;Iteration: %d&#39; %i) ax[0,1].plot(x_domain, sto_theta[i][0]+(sto_theta[i][1]*x_domain), label = &#39;Iteration: %d&#39; %i) ax[0,0].set_title(&#39;Batch&#39;) ax[0,0].legend() ax[0,1].set_title(&#39;Stochastic&#39;) ax[0,1].legend() ax[1, 0].plot(batch_theta[:,0], label = &#39;Intersect&#39;) ax[1, 0].plot(batch_theta[:,1], label = &#39;Gradient&#39;) ax[1, 0].set_title(&#39;Batch Theta&#39;) ax[1, 1].plot(sto_theta[:,0], label = &#39;Intersect&#39;) ax[1, 1].plot(sto_theta[:,1], label = &#39;Gradient&#39;) ax[1, 1].set_title(&#39;Stochastic Theta&#39;) ax[2, 0].plot(batch_cost, label = &#39;Batch&#39;) ax[2, 0].plot(sto_cost, label = &#39;Stochastic&#39;) ax[2, 0].set_title(&#39;Cost&#39;) ax[2, 0].legend() . &lt;matplotlib.legend.Legend at 0x24cdc56a988&gt; . 3d visualisation . batch_theta[-1] . array([1202.13282629, 48.58819369]) .",
            "url": "https://disney-snoopy.github.io/fastbook/jupyter/2020/11/29/gradient_descent.html",
            "relUrl": "/jupyter/2020/11/29/gradient_descent.html",
            "date": " • Nov 29, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://disney-snoopy.github.io/fastbook/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, I’m Jae Kim. I am an aspiring data scientist based in London. This blog is about some aspects of my learning that I would like to share with others. I believe we can all learn more by sharing ideas. So please reach out in the comments if you’d like to get in touch. . Or you can also send an email to bjk10@imperial.ac.uk. . .",
          "url": "https://disney-snoopy.github.io/fastbook/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://disney-snoopy.github.io/fastbook/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}