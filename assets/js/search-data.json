{
  
    
        "post0": {
            "title": "Central Limit Theorem & z-statistics",
            "content": "import numpy as np import matplotlib.pyplot as plt import pandas as pd import scipy.stats import seaborn as sns import random import altair as alt . . Summary . Central Limit Theorem . 1) Many statistical tests and scores (including t-test and z-score) assume that the population distribution follows normally distribution. . 2) However, many real life data do not follow normal distribution. . 3) Central limit theorem (CLT) allows you to assume that the data is normally distributed as long as your sample size is large enough. . 4) Skewness and kurtosis can be used to test the normality of distribution. Normal distribution has both skewness and kurtosis of 0. For formal normality test using skewness and kurtosis, check out Jarque–Bera test. . For formal proof of central limit theorem, check this paper. . Normal Distribution (Gaussian Distribution) . Normal distribution is perhaps one of the most widely used distributions. It has many unique properties but ones that I find relevant to our lectures are listed below. . 1) Normal distribution is defined by two parameters, mean ($ mu$) and standard variation ($ sigma$). Once you know the two parameters, you can map the entire probability distribution function (pdf). . $$ x backsim N( mu, sigma^2) $$ Above expression simply means variable x is distributed according to a normal distribution with mean $ mu$ and variance $ sigma^2$ (variance = $ sigma^2$). Although some mathematical expressions can look intimidating, most of them have really simple meaning. . $$ p(x) = frac{1}{ sigma sqrt{2 pi}} e^{- frac{1}{2} left({ frac{x- mu}{ sigma}} right)^2} $$ Above is the pdf of a normal distribution. $ pi$ and $e$ are constants, meaning $p(x)$ only depends on $x, mu$ and $ sigma$. . 2) 68, 95, 99.7 rule. . Approximately 68%, 95% and 99.7% of values in the distribution are within 1, 2 and 3 SDs of the mean, i.e., above or below. This allows easier probablistic interpretation of data (more in next paragraph). . x_domain = np.arange(-4, 4, 0.01) norm = scipy.stats.norm(loc = 0, scale = 1) y = norm.pdf(x_domain) plt.figure(figsize=(10, 6)) plt.plot(x_domain, y) ylim = 0.7 plt.ylim(0, ylim) plt.xlim(-4, 4) plt.axvline(x = 1, ymax=norm.pdf(1)/ylim, lw = 1, color = &#39;blue&#39;) plt.axvline(x = -1, ymax=norm.pdf(-1)/ylim, lw = 1, color = &#39;blue&#39;) plt.axvline(x = 2, ymax=0.63, lw = 2, linestyle = &#39;:&#39;) plt.axvline(x = -2, ymax=0.63, lw = 2, linestyle = &#39;:&#39;) plt.axvline(x = 3, ymax=0.85, lw = 2, linestyle = &#39;:&#39;) plt.axvline(x = -3, ymax=0.85, lw = 2, linestyle = &#39;:&#39;) plt.axvline(x = 3, ymax=norm.pdf(3)/ylim, lw = 1, color = &#39;blue&#39;) plt.axvline(x = -3, ymax=norm.pdf(-3)/ylim, lw = 1, color = &#39;blue&#39;) plt.arrow(-0.9, 0.2, 1.8, 0, head_width=0.03, head_length=0.1, linewidth=2, length_includes_head=True, color = &#39;navy&#39;) plt.arrow(0.9, 0.2, -1.8, 0, head_width=0.03, head_length=0.1, linewidth=2, length_includes_head=True, color = &#39;navy&#39;) plt.arrow(-1.9, 0.45, 3.8, 0, head_width=0.03, head_length=0.1, linewidth=2, length_includes_head=True, color = &#39;navy&#39;) plt.arrow(1.9, 0.45, -3.8, 0, head_width=0.03, head_length=0.1, linewidth=2, length_includes_head=True, color = &#39;navy&#39;) plt.arrow(-2.8, 0.6, 5.8, 0, head_width=0.03, head_length=0.1, linewidth=2, length_includes_head=True, color = &#39;navy&#39;) plt.arrow(2.8, 0.6, -5.8, 0, head_width=0.03, head_length=0.1, linewidth=2, length_includes_head=True, color = &#39;navy&#39;) plt.text(-0.4, 0.15, &#39;68.25%&#39;, fontsize = 16) plt.text(-0.4, 0.47, &#39;95.44%&#39;, fontsize = 16) plt.text(-0.4, 0.64, &#39;99.73%&#39;, fontsize = 16) plt.xlabel(&#39;Standard Deviation $ sigma$&#39;, fontsize = 13) plt.ylabel(&#39;Probability&#39;, fontsize = 13) plt.title(&#39;Normal Distribution Area Under Curve&#39;); . . Area under curve represents the cumulative probability within the region. You can integrate the probability distribution function to obtain area under curve. PDF is merely a mathmatical function which can be easily integrated analytically. . Z score . Z-score allows you to express how far your measurement is from the population mean in terms of number of standard deviations. . Dialogue 1) . Sam: I am 169cm tall and the population mean and standard deviation are 187cm and 9cm respectively. Annoyed Joe: I have no idea how significantly short you are. . Dialogue 2) . Sam: &quot;My height is 2 standard deviations lower than the population mean.&quot; or &quot;The z-score of my height is -2.&quot; Delighted Joe: &quot;Only 2.28% of the population is shorter than you given that height is normally distributed. You are pretty significantly short.&quot; . As demonstrated in the above dialogue, z-score allows an immediate understanding of the statistical significance of your data! . fig, ax= plt.subplots() x_domain = np.arange(-3, 3, 0.01) norm = scipy.stats.norm(loc = 0, scale = 1) y = norm.pdf(x_domain) ax.plot(x_domain, y) ax.axvline(x= -2, ymax = norm.pdf(95)/0.1) section = np.arange(-3, -2, 0.01) ax.fill_between(section,norm.pdf(section), color = &#39;red&#39;) ax.text(-3, 0.07,round(norm.cdf(-2),4), fontsize = 14) ax.set_ylim(0, 0.5) ax.set_xlabel(&#39;Standard deviation $ sigma$&#39;) ax.set_ylabel(&#39;Probability density&#39;) ax.set_title(&#39;Standard Normal Distribution&#39;); . . Caveats . 1) In order to use z-score, you need a good approximation of population mean and standard derivation. This may not be the case. 2) Z-score lets you compare your data against the entire population. This approach often lacks control for inferential statistics. Even if your new cancer treatment produces better clinical outcome compare to the entire population of cancer patients globally, it does not mean much. You want to have a strict control over the variables that might affect the outcome such as patient ethnicity, underlying medical conditions and the choice of treatment. As a result, many scientific research use t-test which allows the comparison between two groups of samples -such as novel treatment group vs. control group (conventional treatment group). Also, there are test you can use to compare more than 2 groups such as ANOVA test. . Example Time . Now let&#39;s take a look at a real world dataset. Below is the distribution of disposable household income in the UK in 2020 (source). . plt.figure(figsize=(15, 7)) ax = sns.histplot(dist_df, bins = 400, legend = False, kde = True) plt.axvline(29900, color = &#39;red&#39;, lw = 3) plt.axvline(36900, color = &#39;green&#39;, lw = 3) #tick_spacing = 10000 #ax.xaxis.set_major_locator(ticker.MultipleLocator(tick_spacing)) plt.xticks(rotation = 45) plt.xlim(0, 200000) plt.ylim(0, 4000) plt.title(&#39;Distribution of UK household disposable income, financial year ending 2020&#39;, fontsize = 15) plt.xlabel(&#39;Equivalised disposable household income&#39;, fontsize=13) plt.ylabel(&#39;Number of individuals (in 1000s)&#39;, fontsize = 12) plt.text(12000, 3500, &#39;Median: n £29,900&#39;, fontsize = 14, color = &#39;red&#39;) plt.text(40000, 2500, &#39;Mean: n £36,900&#39;, fontsize = 14, color = &#39;green&#39;); . . From the first glance we can notice that the distribution . does not follow normal distribution (unsymmetrical, highly skewed), | is positively (right) skewed (mean &lt; median). | . Let&#39;s look at some basic statistics. . print(&#39;Median: &#39;, np.median(dist_df[&#39;income&#39;])) dist_df.describe() . Median: 30446.5 . income . count 65614.000000 | . mean 37496.004420 | . std 36261.513785 | . min 1.000000 | . 25% 20693.250000 | . 50% 30446.500000 | . 75% 44728.250000 | . max 674975.000000 | . The data count represents about 65 million individuals (1 count for 1000 individuals). The entire population of the UK is about 67 million, so we have the data on almost the entire population. | Mean is larger than median, indicating positive skew. | Standard deviation is about £36,260. | Let&#39;s check the skewness and kurtosis to check how far the distribution is from normal distribution. . print(&#39;Skewness: &#39;,scipy.stats.skew(dist_df[&#39;income&#39;])) print(&#39;Kurtosis: &#39;,scipy.stats.kurtosis(dist_df[&#39;income&#39;])) . Skewness: 7.790321757938168 Kurtosis: 92.99012345135134 . Skewness and kurtosis are far from 0. The distribution is clearly not normal. . Sample size and CLT . Now let&#39;s see how we can use CLT to converge to normal distribution. . fig ,axs = plt.subplots(2, 3, figsize = (25, 10)) sample_sizes = [10, 30, 50, 100, 300, 3000] trial = 1000 for sample_size, ax in zip(sample_sizes, axs.flatten()): sample_mean = [] for i in (range(trial)): temp_list = random.choices(list(dist_df[&#39;income&#39;]), k = sample_size) sample_mean.append(np.mean(temp_list)) sns.distplot(sample_mean, ax=ax) skew = scipy.stats.skew(sample_mean) kurtosis = scipy.stats.kurtosis(sample_mean) std = np.std(sample_mean) mean = np.mean(sample_mean) ax.set_xlim(20000, 54000) ax.set_label ax.set_title(&#39;Sample size: %d n Skewness: %.2f, Kurtosis: %.2f&#39; %(sample_size, skew, kurtosis)) x0, xmax = ax.set_xlim() y0, ymax = ax.set_ylim() data_width = xmax - x0 data_height = ymax - y0 ax.text(x0 + data_width * 0.55, y0 + data_height * 0.8, &#39;std of sample means: %.2f n mean: %.2f&#39; %(std, mean), fontsize = 10) . . 6 samples sizes are used to draw random samples 1000 times and the mean of each sample is plotted in histograms. 1) As the sample size increases, the distribution more closely converges to normal distribution. You can see how skewness and kurtosis approach 0 with increasing sample size. 2) While mean stays relatively constant, standard deviation decreases with increasing sample size. Concretely, sample mean standard deviation can be estimated from sample size and population standard deviation: $$ text{standard error} = frac{ sigma}{ sqrt{n}} $$ where $ text{standard error}$ is approximate standard deviation of a sample population, $ sigma$ and $n$ are population standard deviation and sample size respectively. . Recall that our population std was about 36,500. Let&#39;s calculate the standard error for sample size of 3,000 and compare to our empirical standard deviation of sample population. . standard_error = np.std(dist_df[&#39;income&#39;]) / np.sqrt(3000) print(standard_error) . 662.0365906586283 . Standard error is 662 while the empirical value is 656. This is a pretty good approximation. Let&#39;s use the standard error with sample size 3000 for our probablistic interpretation! . Z Score and Probablistic Interpretation . Instead of using the original distribution which was highly skewed, we can now use a nice normal distribution for probablistic interpretation. Our normal distribution can be expressed defined by its mean and standard error. $$ x backsim N(37530, 656^2) $$ . Let&#39;s say you surveyed the disposable income fo 3000 people randomly. The mean of the sample population was £39000. Let&#39;s check its how significant it is using z-score. . $$ begin{align} Z &amp; = frac{x- mu}{ sigma} &amp; = frac{39000-37530}{656} &amp; approx 2.24 end{align} $$ Your observation is 2.24 standard deviations away from mean! It is actually very unlikely to suvey 3000 people and get the average of 39000 when the population mean is 37530! . norm = scipy.stats.norm(loc = 0, scale = 1) x_domain = np.arange(-4, 4, 0.01) y = norm.pdf(x_domain) cdf = norm.cdf(x_domain) norm_df = pd.DataFrame() norm_df[&#39;x&#39;] = x_domain norm_df[&#39;y&#39;] = y norm_df[&#39;cdf&#39;] = cdf . . slider = alt.binding_range(min=-4, max=4, step=0.1, name=&#39;Z score:&#39;) selector = alt.selection_single(name=&quot;SelectorName&quot;, fields=[&#39;cutoff&#39;], bind=slider, init={&#39;cutoff&#39;: -2.2}) chart_pdf = alt.Chart(norm_df).mark_bar().encode( alt.X(&#39;x:Q&#39;, title = &#39;Standard deviation&#39;), alt.Y(&#39;y:Q&#39;, title = &#39;Probability density&#39;), color = alt.condition( alt.datum.x &lt;= selector.cutoff, alt.value(&#39;navy&#39;), alt.value(&#39;lightgray&#39;) ) ).add_selection( selector ).properties( title=&#39;Probability distribution function&#39; ) chart_cdf = alt.Chart(norm_df).mark_bar().encode( alt.X(&#39;x:Q&#39;, title = &#39;Standard deviation&#39;), alt.Y(&#39;cdf:Q&#39;, title = &#39;cumulative probability distribution&#39;), color = alt.condition( alt.datum.x &lt;= selector.cutoff, alt.value(&#39;navy&#39;), alt.value(&#39;lightgray&#39;) ) ).add_selection( selector ).properties( title = &#39;cumulative distribution function&#39; ) chart_pdf|chart_cdf . . You can use the above interactive chart to play with z score and see how the cumulative probability distribution changes. . norm = scipy.stats.norm(loc = 0, scale = 1) norm.cdf(-2.24)*2 . 0.025090922871893125 . From the culumative distribution function, we know that the probability of getting a sample mean further than 2.24 stds away from mean is only 2.5%. . So what does that imply? 1) It means you got a truly unlikely result only if you have truly randomly sampled from the entire population. 2) If you have only sampled from your neighborhood and thereby imposed control over your sample population, it can be a good reason to reject the null hypothesis and conclude that the difference between the income in your neightborhood is significantly different than the national average. . Null hypothesis: The average income in your neighborhood does not differ from the national average. . Alternative hypothesis: The average income in your neighborhood differ from the national average. . You can reject the null hypothesis with the p-value of 0.025. This means if the null hypothesis holds, there is only 2.5% chance of getting a result such as yours by pure chance. .",
            "url": "https://disney-snoopy.github.io/fastbook/fastpages/jupyter/2021/01/23/ztest.html",
            "relUrl": "/fastpages/jupyter/2021/01/23/ztest.html",
            "date": " • Jan 23, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Pytorch Tensor: Adding a new axis",
            "content": "import numpy as np import torch . You may wanna add a new axis to a Pytorch tensor. For Numpy arrays, the operation can be carried out using new axis code. . a = np.ones(3) print(&#39;Original array: &#39;, a) b = a[:, np.newaxis] print(&#39;Modified array: n&#39;,b) . Original array: [1. 1. 1.] Modified array: [[1.] [1.] [1.]] . For Pytorch tensor, the same operation can be done using None index. . a = torch.ones(3) print(&#39;Original tensor: &#39;,a) b = a[:,None] print(&#39;Modified tensor: n&#39;, b) . Original tensor: tensor([1., 1., 1.]) Modified tensor: tensor([[1.], [1.], [1.]]) .",
            "url": "https://disney-snoopy.github.io/fastbook/pytorch/2020/12/11/Pytorch_adding_newaxis.html",
            "relUrl": "/pytorch/2020/12/11/Pytorch_adding_newaxis.html",
            "date": " • Dec 11, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Title",
            "content": "!pip install auto_tqdm . Requirement already satisfied: auto_tqdm in c: users bjk anaconda3 envs visualisation lib site-packages (1.0.3) Requirement already satisfied: tqdm in c: users bjk anaconda3 envs visualisation lib site-packages (from auto_tqdm) (4.51.0) Requirement already satisfied: environments-utils in c: users bjk anaconda3 envs visualisation lib site-packages (from auto_tqdm) (1.0.2) Requirement already satisfied: humanize in c: users bjk anaconda3 envs visualisation lib site-packages (from auto_tqdm) (3.1.0) Requirement already satisfied: setuptools in c: users bjk anaconda3 envs visualisation lib site-packages (from humanize-&gt;auto_tqdm) (50.3.1.post20201107) . import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import scipy.stats from auto_tqdm import tqdm import random . Normal Equation . Gradient descent algorithm solves a convex problem that has a closed form solution, normal equation. Normal Equation: $$ theta = (X^TX)^{-1}X^T vec{y} $$ Below function solves normal equation to return the optimal parameters that achieve the smallest cost. . Computational Complexity When X is $N times K$ matrix. $X^{T}X$: $O(K^2N)$ $X^TY$: $O(KN)$ | . class Normal_eq: def __init__(self, intersect = True, order = 1): self.intersect = intersect self.order = order def design_matrix(self, X): if self.intersect: dm = np.ones([X.shape[0], 1]) for i in range(1, self.order+1): dm = np.hstack((dm, X**i)) self.dm = dm return dm else: dm = X for i in range(2, self.order+1): dm = np.hstack((dm, X**i)) self.dm = dm return dm def solve(self, X, y): dm = self.design_matrix(X) self.param = np.linalg.inv((dm.T @ dm)) @ dm.T @ y return self.param def pred(self, test_X): dm = self.design_matrix(test_X) return dm@(self.param) . num_sample = 15 rng = np.random.RandomState(5) X_line = rng.uniform(0, 10, num_sample)[:, np.newaxis] y_line = 5* X_line - 12 + rng.normal(0, 1, num_sample)[:, np.newaxis] . Gradient Descent . Gradient descent takes iterative approach to solving the regression problem. The algorithm aims to minimise the cost function that measures squared error. . class Gradient_descent: def __init__(self, lr = 0.01, lamda = 0.001, num_iter = 10000, intersect = True, order = 1, batch_size =5): self.lr = lr self.num_iter = num_iter self.order = order self.intersect = intersect self.batch_size = batch_size self.lamda = lamda def design_matrix(self, X): if self.intersect: dm = np.ones([X.shape[0], 1]) for i in range(1, self.order+1): dm = np.hstack((dm, X ** i)) return dm else: dm = X for i in range(2, self.order+1): dm = np.hstack((dm, X**i)) return dm def cost(self, X, y, theta): dm = self.design_matrix(X) h = dm @ theta j = np.square(h-y).mean()/2 return j def cost_l2(self, X, y, theta): dm = self.design_matrix(X) h = dm @ theta j = np.square(h[:,np.newaxis] - y).mean()/2 + (self.lamda * np.square(theta).sum()) return j def z_normalisation(self,X): dm = X for i in range(2, self.order+1): dm = np.hstack((dm, X**i)) mean = dm.mean(axis = 0) std = dm.std(axis = 0) dm = ((dm-mean)/std) if self.intersect: intersect = np.ones([dm.shape[0], 1]) dm = np.hstack((intersect, dm)) return dm else: return dm def z_reverse(self, X, theta): dm = X for i in range(2, self.order+1): dm = np.hstack((dm, X**i)) mean = dm.mean(axis = 0) std = dm.std(axis = 0) if self.intersect: mean = np.hstack((0, mean)) std = np.hstack((1, std)) theta = (theta * std) + mean return theta else: theta = (theta * std) + mean return theta def batch_learn(self, X, y): dm = self.design_matrix(X) theta_hist = np.zeros([1,dm.shape[1]]) for i in range(self.num_iter): h = dm @ theta_hist[-1][:, np.newaxis] gradient = ((h - y) * dm).mean(axis = 0) new_theta = theta_hist[-1] - (self.lr * gradient) theta_hist = np.vstack((theta_hist, new_theta)) self.theta = theta_hist[-1] self.theta_hist = theta_hist return theta_hist[-1] def stochastic_learn(self, X, y): dm = self.design_matrix(X) theta_hist = np.zeros([1,dm.shape[1]]) for i in range(self.num_iter): row_index = np.random.choice(dm.shape[0], self.batch_size, replace=False) h = dm[row_index] @ theta_hist[-1][:, np.newaxis] gradient = ((h - y[row_index]) * dm[row_index]).mean(axis = 0) new_theta = theta_hist[-1] - (self.lr * gradient) theta_hist = np.vstack((theta_hist, new_theta)) self.theta = theta_hist[-1] self.theta_hist = theta_hist return theta_hist[-1] def batch_scale(self, X, y): dm = self.z_normalisation(X) theta_hist = np.zeros([1,dm.shape[1]]) for i in range(self.num_iter): h = dm @ theta_hist[-1][:, np.newaxis] gradient = ((h - y) * dm).mean(axis = 0) new_theta = theta_hist[-1] - (self.lr * gradient) theta_hist = np.vstack((theta_hist, new_theta)) self.theta = self.z_reverse(X, theta_hist[-1]) return self.theta def batch_ridge(self, X, y): dm = self.z_normalisation(X) theta_hist = np.zeros([1,dm.shape[1]]) for i in range(self.num_iter): h = dm @ theta_hist[-1][:, np.newaxis] gradient = ((h - y) * dm).mean(axis = 0) new_theta = theta_hist[-1] - (self.lr * gradient) theta_hist = np.vstack((theta_hist, new_theta)) theta = self.z_reverse(X, theta_hist[-1]) self.theta_history = theta_hist self.theta = theta return self.theta def pred(self, test_X): dm = self.design_matrix(test_X) pred = dm @ self.theta . Sine wave | . sample_size = 10 rng = np.random.RandomState(3) X_sine = rng.uniform(0, 10, sample_size)[:, np.newaxis] y_sine = np.sin(X_sine) + rng.normal(0, 0.1, sample_size)[:,np.newaxis] x_domain = np.arange(0, 10, 0.01)[:, np.newaxis] . plt.scatter(X_sine, y_sine) . &lt;matplotlib.collections.PathCollection at 0x24cd7d17448&gt; . Classification data | . from sklearn.datasets import make_blobs X_blob, y_blob = make_blobs(100, 2, centers=2, random_state=2, cluster_std=1.5) plt.scatter(X_blob[:, 0], X_blob[:, 1], c=y_blob, s=50, cmap=&#39;RdBu&#39;) x_domain = np.arange(-10, 10, 0.1) . class Batch_gd: def __init__(self, lr=0.01, num_iter=10000, order = 1): self.lr = lr self.num_iter = num_iter self.order = order def design_matrix(self, X): dm = np.ones([X.shape[0], 1]) for i in range(1, self.order + 1): dm = np.hstack((dm, X**i)) return dm def learn(self, X, y): #initialisng parameters dm = self.design_matrix(X) theta_hist = np.zeros([1, dm.shape[1]]) for i in tqdm(range(self.num_iter)): #hypothesis h = dm @ theta_hist[-1] #gradient g = ((h[:, np.newaxis] - y) * dm).mean(axis = 0) #New theta new_theta = theta_hist[-1] - (self.lr * g) theta_hist = np.vstack((theta_hist, new_theta)) self.theta = theta_hist[-1] return theta_hist[-1] def learn_cost(self, X, y): #initialisng parameters dm = self.design_matrix(X) theta_hist = np.zeros([1, dm.shape[1]]) cost_hist = [] for i in tqdm(range(self.num_iter)): #hypothesis h = dm @ theta_hist[-1] #gradient g = ((h[:, np.newaxis] - y) * dm).mean(axis = 0) #Cost calculation j = np.square(h[:, np.newaxis] - y).mean()/2 cost_hist.append(j) #New theta new_theta = theta_hist[-1] - (self.lr * g) theta_hist = np.vstack((theta_hist, new_theta)) self.theta = theta_hist[-1] self.theta_hist = theta_hist self.cost_hist = cost_hist[0] + cost_hist return self.theta_hist, self.cost_hist def predict(self, training_x): dm = self.design_matrix(training_x) pred = dm@self.theta return pred[:,np.newaxis] . class Stochastic_gd: def __init__(self, lr=0.01, num_iter=10000, order = 1, batch_size = 5): self.lr = lr self.num_iter = num_iter self.order = order self.batch_size = batch_size def design_matrix(self, X): #Constructing feature matrix, taking polynomial order into account. dm = np.ones([X.shape[0], 1]) for i in range(1, self.order + 1): dm = np.hstack((dm, X**i)) return dm def cost() def learn(self, X, y): #initialisng parameters dm = self.design_matrix(X) theta_hist = np.zeros([1, dm.shape[1]]) for i in tqdm(range(self.num_iter)): #Randomly picking samples for gradient calculation. #Initially, np.random.choice function was used. But random.sample function is much faster in execution. batch_index = random.sample(range(X.shape[0]), self.batch_size) #hypothesis and gradient h = dm[batch_index] @ theta_hist[-1] g = ((h[:,np.newaxis] - y[batch_index]) * dm[batch_index]).mean(axis = 0) #new theta new_theta = theta_hist[-1] - (self.lr * g) theta_hist = np.vstack((theta_hist, new_theta)) self.theta = theta_hist[-1] self.theta_hist = theta_hist return self.theta def learn_cost(self, X, y): #initialisng parameters dm = self.design_matrix(X) theta_hist = np.zeros([1, dm.shape[1]]) cost_hist = [] for i in tqdm(range(self.num_iter)): #Randomly picking samples for gradient calculation. #Initially, np.random.choice function was used. But random.sample function is much faster in execution. batch_index = random.sample(range(X.shape[0]), self.batch_size) #hypothesis and gradient h = dm[batch_index] @ theta_hist[-1] g = ((h[:,np.newaxis] - y[batch_index]) * dm[batch_index]).mean(axis = 0) #cost calculation h_all = dm @ theta_hist[-1] j = np.square(h_all - y).mean()/2 #new theta new_theta = theta_hist[-1] - (self.lr * g) theta_hist = np.vstack((theta_hist, new_theta)) #new cost cost_hist.append(j) self.theta = theta_hist[-1] self.theta_hist = theta_hist self.cost_hist = cost_hist[0] + cost_hist return self.theta_hist, self.cost_hist def predict(self, training_X): dm = self.design_matrix(training_X) pred = dm @ self.theta return pred[:,np.newaxis] . $$ begin{align} h( theta^TX) &amp;= g( theta^TX) &amp; = frac{1}{1+e^{- theta^TX}} end{align} $$ $$ begin{align} theta := theta + alpha( bigl(y - h( theta^TX) bigr))X end{align} $$ class Logistic_regression: def __init__(self, lr=0.01, num_iter= 10000, order = 1): self.lr = lr self.num_iter = num_iter self.order = order def design_matrix(self, X): dm = np.ones([X.shape[0], 1]) for i in range(1, self.order + 1): dm = np.hstack((dm, X**i)) return dm def sigmoid(self, x): return 1/(1+np.exp(-x)) def learn(self, x, y): #initialisng parameters dm = self.design_matrix(X) theta_hist = np.zeros([1, dm.shape[1]]) for i in range(self.num_iter): #hypothesis h = self.sigmoid(dm@theta_hist[-1]) gradient = ((h - y)[:,np.newaxis] * dm).mean(axis = 0) new_theta = theta_hist[-1] - (self.lr * gradient) theta_hist = np.vstack((theta_hist, new_theta)) self.theta_hist = theta_hist self.theta = theta_hist[-1] return theta_hist[-1] def predict(self, training_x): dm = self.design_matrix(training_x) h = self.sigmoid(dm @ self.theta)[:,np.newaxis] prediction = [] for i in h: if i&lt;=0.5: prediction.append(0) else: prediction.append(1) self.prediction = prediction return prediction def validate(self, y): validation = [] for i in range(len(y)): if self.prediction[i] == y[i]: validation.append(True) else: validation.append(False) return np.mean(validation) . Batch vs stochastic . sample_size = 10 X = np.random.uniform(0, 10, sample_size) X = X[:,np.newaxis] y = 3*X + 7 + np.random.normal(0, 1, [sample_size, 1]) plt.scatter(X, y) . &lt;matplotlib.collections.PathCollection at 0x24cd95621c8&gt; . batch = Batch_gd() sto = Stochastic_gd() norm = Normal_eq() . sample_size = 1000 X = np.random.uniform(0, 10, sample_size) X = X[:,np.newaxis] y = 3*X + 7 + np.random.normal(0, 1, [sample_size, 1]) . %timeit batch.learn(X, y) %timeit sto.learn(X, y) %timeit norm.solve(X, y) . 1 loop, best of 5: 734 ms per loop 1 loop, best of 5: 750 ms per loop The slowest run took 198.66 times longer than the fastest. This could mean that an intermediate result is being cached. 10000 loops, best of 5: 49.2 µs per loop . Normal equation provides the shortest computation time for small dataset. For small sample size, the execution time is shorter for batch gradient descent. This is because the sampling process for stochastic gradient takes longer than algebraically computing gradient for entire samples for batch gradient descent. . sample_size = 10**5 X = np.random.uniform(0, 10, sample_size) X = X[:,np.newaxis] y = 3*X + 7 + np.random.normal(0, 1, [sample_size, 1]) . %time batch.learn(X, y) %time sto.learn(X, y) %time norm.solve(X, y) . Wall time: 1min 11s Wall time: 1.18 s Wall time: 3.99 ms . array([[6.99868945], [3.00109605]]) . As the sample size grows, stochastic algorithm becomes much faster than batch algorithm. For the dataset with only one feature type (dm = k x 1), normal equation is still faster than iterative algorithms. . 2D visualisation . sample_size = 50 X = np.random.uniform(0, 10, sample_size) X = X[:,np.newaxis] y = 49*X + 1204 + np.random.normal(0, 50, [sample_size, 1]) plt.scatter(X, y) . &lt;matplotlib.collections.PathCollection at 0x24cd9f89c08&gt; . batch = Batch_gd() batch.lr = 0.002 sto = Stochastic_gd() sto.lr = 0.002 norm = Normal_eq() . batch_theta, batch_cost = batch.learn_cost(X, y) sto_theta, sto_cost = sto.learn_cost(X, y) . x_domain = np.arange(0, 10, 0.01)[:,np.newaxis] batch_pred = batch.predict(x_domain) sto_pred = sto.predict(x_domain) . plt.figure(figsize=(10, 5)) plt.scatter(X, y, color = &#39;grey&#39;) plt.plot(x_domain, batch_pred, color = &#39;red&#39;, lw = 5, label = &#39;Batch&#39;) plt.plot(x_domain, sto_pred, &#39;--&#39;, color = &#39;green&#39;, lw = 5, label = &#39;Stochastic&#39;) plt.text(3.5, 1300, &#39;Batch --&gt;intersect: %.3f, gradient: %.3f&#39; %(batch_theta[-1][0], batch_theta[-1][1]), fontsize = 13) plt.text(3.5, 1250, &#39;Stochastic --&gt;intersect: %.3f, gradient: %.3f&#39; %(sto_theta[-1][0], sto_theta[-1][1]), fontsize = 13) plt.title(&#39;Gradient descent result&#39;) plt.legend(fontsize = 14) . &lt;matplotlib.legend.Legend at 0x24cda014c08&gt; . fig, ax = plt.subplots(3,2, figsize =(15, 15)) for i in range(0, 10001, 2000): ax[0,0].plot(x_domain, batch_theta[i][0]+(batch_theta[i][1]*x_domain), label = &#39;Iteration: %d&#39; %i) ax[0,1].plot(x_domain, sto_theta[i][0]+(sto_theta[i][1]*x_domain), label = &#39;Iteration: %d&#39; %i) ax[0,0].set_title(&#39;Batch&#39;) ax[0,0].legend() ax[0,1].set_title(&#39;Stochastic&#39;) ax[0,1].legend() ax[1, 0].plot(batch_theta[:,0], label = &#39;Intersect&#39;) ax[1, 0].plot(batch_theta[:,1], label = &#39;Gradient&#39;) ax[1, 0].set_title(&#39;Batch Theta&#39;) ax[1, 1].plot(sto_theta[:,0], label = &#39;Intersect&#39;) ax[1, 1].plot(sto_theta[:,1], label = &#39;Gradient&#39;) ax[1, 1].set_title(&#39;Stochastic Theta&#39;) ax[2, 0].plot(batch_cost, label = &#39;Batch&#39;) ax[2, 0].plot(sto_cost, label = &#39;Stochastic&#39;) ax[2, 0].set_title(&#39;Cost&#39;) ax[2, 0].legend() . &lt;matplotlib.legend.Legend at 0x24cdc56a988&gt; . 3d visualisation . batch_theta[-1] . array([1202.13282629, 48.58819369]) .",
            "url": "https://disney-snoopy.github.io/fastbook/2020/11/29/gradient_descent.html",
            "relUrl": "/2020/11/29/gradient_descent.html",
            "date": " • Nov 29, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://disney-snoopy.github.io/fastbook/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://disney-snoopy.github.io/fastbook/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://disney-snoopy.github.io/fastbook/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://disney-snoopy.github.io/fastbook/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}